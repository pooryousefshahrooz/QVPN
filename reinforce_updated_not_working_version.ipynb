{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, '../')\n",
    "from game import CFRRL_Game\n",
    "from model import Model\n",
    "# from network import Network\n",
    "# from config import get_config\n",
    "from solver import Solver\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "FLAGS = flags.FLAGS\n",
    "# flags.DEFINE_integer('num_agents',1, 'number of agents')\n",
    "# flags.DEFINE_string('baseline', 'avg', 'avg: use average reward as baseline, best: best reward as baseline')\n",
    "# flags.DEFINE_integer('num_iter', 20, 'Number of iterations each agent would run')\n",
    "# print(FLAGS.num_agents)\n",
    "import pdb\n",
    "# num_agents = 1\n",
    "# num_iter =20\n",
    "# baseline = \"avg\"\n",
    "# pdb.set_trace()\n",
    "GRADIENTS_CHECK=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560282a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5866ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21efc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RL:\n",
    "    def __init__(self,config,start = 0):\n",
    "        self.num_agents = 1\n",
    "        self.num_iter =config.rl_batch_size\n",
    "        self.epoch_numbers = 1\n",
    "        self.baseline = \"avg\"\n",
    "        self.ckpt = ''\n",
    "        self.training_testing_switching_file = config.training_testing_switching_file\n",
    "        \n",
    "        #we use lock to write on the training and testing switching file \n",
    "        self.lock = threading.Lock()\n",
    "        self.value = start\n",
    "        \n",
    "    \n",
    "    def get_testing_flag(self):\n",
    "        \n",
    "        f = open(self.training_testing_switching_file, 'r')\n",
    "           \n",
    "        for line in f:\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "                line = line.rstrip()\n",
    "                link = line.split('\\t')\n",
    "                step,testing_flag = link\n",
    "                print(type(testing_flag),testing_flag)\n",
    "                if testing_flag==\"True\":\n",
    "                    return step,True\n",
    "                else:\n",
    "                    return step,False\n",
    "        \n",
    "        f.close()\n",
    "       \n",
    "    def set_testing_flag(self,last_step,training_flag):\n",
    "        print(\" **************************************** we are going to set the flag of testing to %s with step %s \"%(training_flag,last_step))\n",
    "#         self.lock.acquire()\n",
    "        if not os.path.isfile(self.training_testing_switching_file):\n",
    "            f = open(self.training_testing_switching_file, \"w\")\n",
    "            f.write(str(last_step)+\"\\t\"+str(\"True\")+\"\\n\")\n",
    "            f.close()\n",
    "        with open(self.training_testing_switching_file, \"w\") as file_object:\n",
    "            if training_flag:\n",
    "                file_object.write(str(last_step)+\"\\t\"+str(\"True\")+\"\\n\")\n",
    "            else:\n",
    "                file_object.write(str(last_step)+\"\\t\"+str(\"False\")+\"\\n\")\n",
    "        #file_object.close()\n",
    "#         self.lock.release()\n",
    "        \n",
    "        try:\n",
    "            f = open(self.training_testing_switching_file+\".txt\", 'r')\n",
    "        except:\n",
    "            f = open(self.training_testing_switching_file, 'r')\n",
    "        for line in f:\n",
    "            if line:\n",
    "                line = line.strip()\n",
    "                link = line.split('\\t')\n",
    "                #print(line,link)\n",
    "                step,testing_flag = link\n",
    "                print(\"this is what we read from file \",line)\n",
    "        print(\"we set the flag of testing to %s with step %s \"%(training_flag,last_step))\n",
    "    def central_agent(self,config, topology_name,game, model_weights_queues, experience_queues):\n",
    "        model = Model(config,topology_name, game.state_dims, game.action_dim, game.max_moves, master=True)\n",
    "        model.save_hyperparams(config)\n",
    "        start_step = model.restore_ckpt()\n",
    "        for step in tqdm(range(start_step, self.epoch_numbers), ncols=70, initial=start_step):\n",
    "            model.ckpt.step.assign_add(1)\n",
    "            model_weights = model.model.get_weights()\n",
    "\n",
    "#             for i in range(FLAGS.num_agents):\n",
    "            for i in range(self.num_agents):\n",
    "                model_weights_queues[i].put(model_weights)\n",
    "\n",
    "            if config.method == 'actor_critic':\n",
    "                #assemble experiences from the agents\n",
    "                s_batch = []\n",
    "                a_batch = []\n",
    "                r_batch = []\n",
    "\n",
    "#                 for i in range(FLAGS.num_agents):\n",
    "                for i in range(self.num_agents):\n",
    "                    s_batch_agent, a_batch_agent, r_batch_agent = experience_queues[i].get()\n",
    "\n",
    "#                     assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    assert len(s_batch_agent) == self.num_iter, \\\n",
    "                        (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent))\n",
    "\n",
    "                    s_batch += s_batch_agent\n",
    "                    a_batch += a_batch_agent\n",
    "                    r_batch += r_batch_agent\n",
    "\n",
    "                assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "                #used shared RMSProp, i.e., shared g\n",
    "                actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "                value_loss, entropy, actor_gradients, critic_gradients = model.actor_critic_train(np.array(s_batch), \n",
    "                                                                        actions, \n",
    "                                                                        np.array(r_batch).astype(np.float32), \n",
    "                                                                        config.entropy_weight)\n",
    "\n",
    "                if GRADIENTS_CHECK:\n",
    "                    for g in range(len(actor_gradients)):\n",
    "                        assert np.any(np.isnan(actor_gradients[g])) == False, ('actor_gradients', s_batch, a_batch, r_batch, entropy)\n",
    "                    for g in range(len(critic_gradients)):\n",
    "                        assert np.any(np.isnan(critic_gradients[g])) == False, ('critic_gradients', s_batch, a_batch, r_batch)\n",
    "\n",
    "                if step % config.save_step == config.save_step -1:\n",
    "                    print(\"going to store checkpoint in training step \",step)\n",
    "                    testing_flag = True\n",
    "                    last_step = step\n",
    "                    while(testing_flag):\n",
    "                        \n",
    "                        last_step,testing_flag = self.get_testing_flag()\n",
    "                        \n",
    "                        if not testing_flag:\n",
    "                            print(\"********************************** training and the flag is %s which means we can save and continious\"%(testing_flag))\n",
    "                        else:\n",
    "                            print(\"training and the flag is %s which means we have to wait!\"%(testing_flag))\n",
    "                            time.sleep(2)\n",
    "                        \n",
    "                        last_step= int(last_step)\n",
    "                    model.save_ckpt(_print=True)\n",
    "\n",
    "                    #log training information\n",
    "                    actor_learning_rate = model.lr_schedule(model.actor_optimizer.iterations.numpy()).numpy()\n",
    "                    avg_value_loss = np.mean(value_loss)\n",
    "                    avg_reward = np.mean(r_batch)\n",
    "                    avg_entropy = np.mean(entropy)\n",
    "\n",
    "                    model.inject_summaries({\n",
    "                        'learning rate': actor_learning_rate,\n",
    "                        'value loss': avg_value_loss,\n",
    "                        'avg reward': avg_reward,\n",
    "                        'avg entropy': avg_entropy\n",
    "                        }, step)\n",
    "                    print('lr:%f, value loss:%f, avg reward:%f, avg entropy:%f step %s'%(actor_learning_rate, avg_value_loss, avg_reward, avg_entropy,step))\n",
    "                    self.set_testing_flag(step,True)\n",
    "                    testing_flag = True\n",
    "                    while(testing_flag):\n",
    "                        last_step,testing_flag = self.get_testing_flag() \n",
    "                        if not testing_flag:\n",
    "                            print(\"training and the flag is %s which means we can save\"%(testing_flag))\n",
    "                        else:\n",
    "                            print(\"training and the flag is %s which means we have to wait!\"%(testing_flag))\n",
    "                            time.sleep(2)\n",
    "                        last_step= int(last_step)\n",
    "                    \n",
    "            elif config.method == 'pure_policy':\n",
    "                #assemble experiences from the agents\n",
    "                s_batch = []\n",
    "                a_batch = []\n",
    "                r_batch = []\n",
    "                ad_batch = []\n",
    "\n",
    "#                 for i in range(FLAGS.num_agents):\n",
    "                for i in range(self.num_agents):\n",
    "                    s_batch_agent, a_batch_agent, r_batch_agent, ad_batch_agent = experience_queues[i].get()\n",
    "\n",
    "#                     assert len(s_batch_agent) == FLAGS.num_iter, \\\n",
    "                    assert len(s_batch_agent) == self.num_iter, \\\n",
    "                        (len(s_batch_agent), len(a_batch_agent), len(r_batch_agent), len(ad_batch_agent))\n",
    "\n",
    "                    s_batch += s_batch_agent\n",
    "                    a_batch += a_batch_agent\n",
    "                    r_batch += r_batch_agent\n",
    "                    ad_batch += ad_batch_agent\n",
    "\n",
    "                assert len(s_batch)*game.max_moves == len(a_batch)\n",
    "                #used shared RMSProp, i.e., shared g\n",
    "                actions = np.eye(game.action_dim, dtype=np.float32)[np.array(a_batch)]\n",
    "                entropy, gradients = model.policy_train(np.array(s_batch), \n",
    "                                                          actions, \n",
    "                                                          np.vstack(ad_batch).astype(np.float32), \n",
    "                                                          config.entropy_weight)\n",
    "\n",
    "                if GRADIENTS_CHECK:\n",
    "                    for g in range(len(gradients)):\n",
    "                        assert np.any(np.isnan(gradients[g])) == False, (s_batch, a_batch, r_batch)\n",
    "\n",
    "                if step % config.save_step == config.save_step -1:\n",
    "                    print(\"2 going to store checkpoint in training step \",step)\n",
    "                    testing_flag = True\n",
    "                    last_step = step\n",
    "                    while(testing_flag):\n",
    "                        \n",
    "                        last_step,testing_flag = self.get_testing_flag()\n",
    "                       \n",
    "                        \n",
    "                        if not testing_flag:\n",
    "                            print(\"training and the flag is %s which means we can save\"%(testing_flag))\n",
    "                        else:\n",
    "                            print(\"training and the flag is %s which means we have to wait!\"%(testing_flag))\n",
    "                            time.sleep(2)\n",
    "                        last_step= int(last_step)\n",
    "                    model.save_ckpt(_print=True)\n",
    "\n",
    "                    #log training information\n",
    "                    learning_rate = model.lr_schedule(model.optimizer.iterations.numpy()).numpy()\n",
    "                    avg_reward = np.mean(r_batch)\n",
    "                    avg_advantage = np.mean(ad_batch)\n",
    "                    avg_entropy = np.mean(entropy)\n",
    "                    model.inject_summaries({\n",
    "                        'learning rate': learning_rate,\n",
    "                        'avg reward': avg_reward,\n",
    "                        'avg advantage': avg_advantage,\n",
    "                        'avg entropy': avg_entropy\n",
    "                        }, step)\n",
    "                    print('lr:%f, avg reward:%f, avg advantage:%f, avg entropy:%f step %s'%(learning_rate, avg_reward, avg_advantage, avg_entropy,step))\n",
    "                    self.set_testing_flag(step,True)\n",
    "                    testing_flag = True\n",
    "                    while(testing_flag):\n",
    "                        last_step,testing_flag = self.get_testing_flag() \n",
    "                        if not testing_flag:\n",
    "                            print(\"training and the flag is %s which means we can save\"%(testing_flag))\n",
    "                        else:\n",
    "                            print(\"training and the flag is %s which means we have to wait!\"%(testing_flag))\n",
    "                            time.sleep(2)\n",
    "                        last_step= int(last_step)\n",
    "                    \n",
    "    def agent(self,agent_id, config, game,network, wk_subset, model_weights_queue, experience_queue):\n",
    "        random_state = np.random.RandomState(seed=agent_id)\n",
    "        model = Model(config,network.topology_name, game.state_dims, game.action_dim, game.max_moves, master=False)\n",
    "        solver = Solver()\n",
    "        # initial synchronization of the model weights from the coordinator \n",
    "        model_weights = model_weights_queue.get()\n",
    "        model.model.set_weights(model_weights)\n",
    "\n",
    "        idx = 0\n",
    "        s_batch = []\n",
    "        a_batch = []\n",
    "        r_batch = []\n",
    "        if config.method == 'pure_policy':\n",
    "            ad_batch = []\n",
    "        run_iteration_idx = 0\n",
    "        num_wks = len(wk_subset)\n",
    "        random_state.shuffle(wk_subset)\n",
    "        run_iterations = self.num_iter\n",
    "        print(\"we are in agent \")\n",
    "        while True:\n",
    "            wk_idx = wk_subset[idx]\n",
    "            print(\"training work load %s from %s \"%(wk_idx,len(wk_subset)))\n",
    "            network.get_each_user_all_paths(wk_idx,False)\n",
    "            #state\n",
    "            state = game.get_state(wk_idx,network,False)\n",
    "            s_batch.append(state)\n",
    "            #action\n",
    "            if config.method == 'actor_critic':    \n",
    "                policy = model.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "            elif config.method == 'pure_policy':\n",
    "                policy = model.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "            #print(\"np.count_nonzero(policy) >= game.max_moves, (policy, state)\",np.count_nonzero(policy) , game.max_moves)\n",
    "            #print(\"(policy that are all paths %s, state %s)\"%(policy, state))\n",
    "            \n",
    "            assert np.count_nonzero(policy) >= game.max_moves, (policy, state)\n",
    "            actions = random_state.choice(game.action_dim, game.max_moves, p=policy, replace=False)\n",
    "#             print(\"we selected top paths %s which are %s\"%(len(actions),actions))\n",
    "            \n",
    "            for a in actions:\n",
    "                a_batch.append(a)\n",
    "\n",
    "            #reward\n",
    "            reward = game.reward(wk_idx,network,actions,solver)\n",
    "            \n",
    "            print(\"training for workload %s got reward %s \"%(wk_idx,reward))\n",
    "            #print(\"reward is \",reward)\n",
    "            r_batch.append(reward)\n",
    "\n",
    "            if config.method == 'pure_policy':\n",
    "                #advantage\n",
    "                if config.baseline == 'avg':\n",
    "                    ad_batch.append(game.advantage(wk_idx, reward))\n",
    "                    game.update_baseline(wk_idx, reward)\n",
    "                elif config.baseline == 'best':\n",
    "                    best_actions = policy.argsort()[-game.max_moves:]\n",
    "                    best_reward = game.reward(wk_idx, best_actions)\n",
    "                    ad_batch.append(reward - best_reward)\n",
    "\n",
    "            run_iteration_idx += 1\n",
    "            if run_iteration_idx >= run_iterations:\n",
    "                # Report experience to the coordinator                          \n",
    "                if config.method == 'actor_critic':    \n",
    "                    experience_queue.put([s_batch, a_batch, r_batch])\n",
    "                elif config.method == 'pure_policy':\n",
    "                    experience_queue.put([s_batch, a_batch, r_batch, ad_batch])\n",
    "\n",
    "                #print('report', agent_id)\n",
    "\n",
    "                # synchronize the network parameters from the coordinator\n",
    "                model_weights = model_weights_queue.get()\n",
    "                model.model.set_weights(model_weights)\n",
    "\n",
    "                del s_batch[:]\n",
    "                del a_batch[:]\n",
    "                del r_batch[:]\n",
    "                if config.method == 'pure_policy':\n",
    "                    del ad_batch[:]\n",
    "                run_iteration_idx = 0\n",
    "            # Update idx\n",
    "            idx += 1\n",
    "            if idx == num_wks:\n",
    "               random_state.shuffle(wk_subset)\n",
    "               idx = 0\n",
    "\n",
    "    def train(self,config,network):\n",
    "        #cpu only\n",
    "        tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "        self.training_testing_switching_file = config.training_testing_switching_file\n",
    "        #tf.get_logger().setLevel('INFO')\n",
    "        #tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "        #config = get_config(FLAGS) or FLAGS\n",
    "        \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "        # we se the state dimention and action dimention\n",
    "        game = CFRRL_Game(config,network)\n",
    "        self.epoch_numbers = config.max_step\n",
    "        self.set_testing_flag(1,False)\n",
    "        model_weights_queues = []\n",
    "        experience_queues = []\n",
    "#                                             if FLAGS.num_agents == 0 or FLAGS.num_agents >= mp.cpu_count():\n",
    "#                                                 FLAGS.num_agents = mp.cpu_count() - 1\n",
    "#                                             print('Agent num: %d, iter num: %d\\n'%(FLAGS.num_agents+1, FLAGS.num_iter))\n",
    "#                                             for _ in range(FLAGS.num_agents):\n",
    "        if self.num_agents == 0 or self.num_agents >= mp.cpu_count():\n",
    "            self.num_agents = mp.cpu_count() - 1\n",
    "        print('Agent num: %d, iter num: %d\\n'%(self.num_agents+1, self.num_iter))\n",
    "        for _ in range(self.num_agents):\n",
    "            model_weights_queues.append(mp.Queue(1))\n",
    "            experience_queues.append(mp.Queue(1))\n",
    "\n",
    "#                                             tm_subsets = np.array_split(game.wk_indexes, FLAGS.num_agents)\n",
    "        wk_subsets = np.array_split(game.wk_indexes, self.num_agents)\n",
    "\n",
    "        coordinator = mp.Process(target=self.central_agent, args=(config,network.topology_name, game, model_weights_queues, experience_queues))\n",
    "\n",
    "        coordinator.start()\n",
    "\n",
    "        agents = []\n",
    "#                                             for i in range(FLAGS.num_agents):\n",
    "        for i in range(self.num_agents):\n",
    "            agents.append(mp.Process(target=self.agent, args=(i, config, game, network,wk_subsets[i], model_weights_queues[i], experience_queues[i])))\n",
    "\n",
    "        #for i in range(FLAGS.num_agents):\n",
    "        for i in range(self.num_agents):\n",
    "            agents[i].start()\n",
    "\n",
    "        coordinator.join()\n",
    "\n",
    "    def sim(self,config, model,network,solver, game,wk_idx):\n",
    "        #for wk_idx in game.wk_indexes:\n",
    "        #print(\"*************************** getting the state for work load %s ****************** \"%(wk_idx))\n",
    "        state = game.get_state(wk_idx,network,True)\n",
    "        if config.method == 'actor_critic':\n",
    "            policy = model.actor_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        elif config.method == 'pure_policy':\n",
    "            policy = model.policy_predict(np.expand_dims(state, 0)).numpy()[0]\n",
    "        actions = policy.argsort()[-game.max_moves:]\n",
    "        egr = game.evaluate(wk_idx,network,solver,\"RL\",actions) \n",
    "        return actions,egr\n",
    "    def test(self,config,network):\n",
    "        self.epoch_numbers = config.max_step\n",
    "        #network.get_each_user_all_paths_across_wks()\n",
    "        last_step = 1\n",
    "        new_last_step = 1\n",
    "        testing_flag = False\n",
    "        solver = Solver()\n",
    "        print(\"****************** testing ********************************\")\n",
    "        while(last_step<self.epoch_numbers):\n",
    "            #print(\"going to call self.get_testing_flag(last_step) ******************** \")\n",
    "            while(not testing_flag):\n",
    "                \n",
    "                new_last_step,testing_flag = self.get_testing_flag()\n",
    "                print(\" testing we read the file and it was flag %s step %s\"%(testing_flag,new_last_step))\n",
    "                \n",
    "                    #pass\n",
    "    #             print(\"******* this is our step and flag %s %s **********\"%(last_step,testing_flag))\n",
    "                if not testing_flag:\n",
    "                    print(\"testing and the flag is %s which means we have to wait!\"%(testing_flag))\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(\"testing and the flag is %s which means we can test :)\"%(testing_flag))\n",
    "                    \n",
    "                new_last_step = int(new_last_step)\n",
    "            \n",
    "            if new_last_step>last_step:\n",
    "                \n",
    "                last_step = new_last_step\n",
    "                self.ckpt = ''\n",
    "                #Using cpu for testing\n",
    "                tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "                tf.get_logger().setLevel('INFO')\n",
    "                \n",
    "                each_wk_scheme_egr ={}\n",
    "                each_wk_idx_optimal = {}\n",
    "                \"\"\"we first find the candidate paths and use it for action dimention\"\"\"\n",
    "                # we set the state dimention and action dimention\n",
    "                game = CFRRL_Game(config, network)\n",
    "                model = Model(config,network.topology_name, game.state_dims, game.action_dim, game.max_moves)\n",
    "        #         last_chckpoint = model.restore_ckpt(FLAGS.ckpt)\n",
    "                last_chckpoint = model.restore_ckpt(self.ckpt)\n",
    "                model = Model(config,network.topology_name, game.state_dims, game.action_dim, game.max_moves)\n",
    "        #         model = Model(config, game.state_dims, game.action_dim, game.max_moves)\n",
    "        #             current_chckpoint = model.restore_ckpt(FLAGS.ckpt)\n",
    "                current_chckpoint = model.restore_ckpt(self.ckpt)\n",
    "                if config.method == 'actor_critic':\n",
    "                    learning_rate = model.lr_schedule(model.actor_optimizer.iterations.numpy()).numpy()\n",
    "                elif config.method == 'pure_policy':\n",
    "                    learning_rate = model.lr_schedule(model.optimizer.iterations.numpy()).numpy()\n",
    "                print('\\nstep %d, learning rate: %f\\n'% (current_chckpoint, learning_rate))\n",
    "                time_in_seconds = time.time()\n",
    "                for wk_idx in range(len(game.testing_wk_indexes)):\n",
    "                    \n",
    "                    #print(\" *** going to get the paths of all users in workload %s out of %s ***\"%(wk_idx,len(game.testing_wk_indexes)))\n",
    "                    network.get_each_user_all_paths(wk_idx,False)\n",
    "                    actions,rl_egr= self.sim(config,model,network,solver,game,wk_idx)\n",
    "                    print(\"testing for workload %s got egr %s \"%(wk_idx,rl_egr))\n",
    "                    network.save_results(wk_idx,config,False,True,False,False,new_last_step,rl_egr,0,0,time_in_seconds)\n",
    "                    print(\"we saved the results in file!\")\n",
    "                    if config.save_rl_results_for_initialization and new_last_step in config.set_of_epoch_for_saving_rl_results_for_ga:\n",
    "                        print(\"we have wk %s epoch number %s and target to save path information are %s\"%(wk_idx,new_last_step,config.set_of_epoch_for_saving_rl_results_for_ga))\n",
    "                        print(\"we are going to save\")\n",
    "                        network.save_rl_results_for_genetic_initialization(config,wk_idx,new_last_step,rl_egr)\n",
    "                    else:\n",
    "                        print(\"no save! to initialize genetic epoch number %s worklosd %s from %s \"%(new_last_step,wk_idx,len(game.testing_wk_indexes)))\n",
    "                    \n",
    "                    if new_last_step%100==0:\n",
    "                        print(\" ****epoch #\",new_last_step,\"# paths\",network.num_of_paths,\"wk_idx\",wk_idx,\n",
    "                                    \"RL\",rl_egr)\n",
    "                self.set_testing_flag(new_last_step,False)\n",
    "                    \n",
    "            else:\n",
    "                print(\"the flag for testing is set to  %s but we already have trained for this epoch number %s \"%(testing_flag,new_last_step))\n",
    "                time.sleep(3)\n",
    "                new_last_step,testing_flag = self.get_testing_flag()\n",
    "                new_last_step = int(new_last_step)\n",
    "                print(\" testing 2: we read the file and it was flag %s step %s\"%(testing_flag,new_last_step))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl = RL()\n",
    "# rl.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# tm_indexes = np.arange(0, 100)\n",
    "# print(tm_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e37aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
