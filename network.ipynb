{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import groupby\n",
    "import time\n",
    "import math as mt\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from solver import Solver\n",
    "from genetic_algorithm import Genetic_algorithm\n",
    "from reinforce import RL\n",
    "from datetime import datetime\n",
    "import pdb\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from threading import Thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37585b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,config,purification_object ,topology_file,training_flag):\n",
    "        self.data_dir = './data/'\n",
    "        self.topology_file = self.data_dir+topology_file\n",
    "        self.work_load_file_extension = config.work_load_file\n",
    "        self.topology_name = topology_file\n",
    "        self.toplogy_wk_scheme_result  = config.toplogy_wk_scheme_result_file\n",
    "        self.training = training_flag\n",
    "        self.set_E = []\n",
    "        \n",
    "        self.pair_id = 0\n",
    "        self.number_of_flows = 1\n",
    "        \n",
    "        self.toplogy_wk_paths_for_each_candidate_size = config.toplogy_wk_paths_for_each_candidate_size\n",
    "        # this is to have minimum rate constraint or not\n",
    "        self.optimization_problem_with_minimum_rate = config.optimization_problem_with_minimum_rate\n",
    "        \n",
    "        # this is to have maximum rate constraint or not\n",
    "        self.optimization_problem_with_maximum_rate = config.optimization_problem_with_maximum_rate\n",
    "        self.max_flow_rate = config.max_flow_rate\n",
    "        \n",
    "        self.num_of_paths = int(config.num_of_paths)\n",
    "        # we use these two metrics to engineer our original topology\n",
    "        self.target_long_link=1\n",
    "        self.repeater_placement_distance = 1\n",
    "        \n",
    "        # minimum rate for flowsgenetic\n",
    "        self.min_flow_rate = config.min_flow_rate\n",
    "        # we set this flag if we want to store the rates served to each flow\n",
    "        \n",
    "        self.get_flow_rates_info = config.get_flow_rates_info\n",
    "        \n",
    "        # we have a fixed distilation for the greedy-path-selection schemes\n",
    "        self.distilation_scheme_for_huristic = config.distilation_scheme_for_huristic\n",
    "        \n",
    "        # this is for checking the results of what epoch should be used in genetic algorthm initialization\n",
    "        self.genetic_algorithm_initial_population_rl_epoch_number =config.genetic_algorithm_initial_population_rl_epoch_number\n",
    "        \n",
    "        # I use this to read the specific minimum rate constraints in file\n",
    "        self.each_flow_minimum_rate = {}\n",
    "        self.each_flow_minimum_rate_value_file = config.each_flow_minimum_rate_value_file\n",
    "        \n",
    "        # we use this to get the results of the training with how many training work loads\n",
    "        self.number_of_training_wks = config.number_of_training_wks\n",
    "        \n",
    "        # this is the file that have the results fo rl training after different numbers of training epoch and we use for initialization the genetic algorithm        \n",
    "        self.toplogy_wk_rl_for_initialization_ga_result_file = config.toplogy_wk_rl_for_initialization_ga_result_file\n",
    "        self.number_of_flow_set = config.number_of_flow_set\n",
    "        \n",
    "        self.each_wk_k_pair_id  ={}\n",
    "        self.each_wk_k_id_pair ={}\n",
    "        self.each_edge_distance = {}\n",
    "        self.set_of_paths = {}\n",
    "        self.each_u_paths = {}\n",
    "        self.each_wk_k_user_pair_id_permitted_unique_paths ={}\n",
    "        self.each_u_all_real_paths = {}\n",
    "        self.each_u_all_real_disjoint_paths = {}\n",
    "        self.each_u_paths = {}\n",
    "        self.nodes = []\n",
    "        \n",
    "        self.path_counter_id = 0\n",
    "        self.pair_id = 0\n",
    "        self.q_value = 1\n",
    "        self.each_node_q_value = {}\n",
    "        self.each_u_weight={}\n",
    "        self.each_path_legth = {}\n",
    "        self.K= []\n",
    "        self.each_k_u_all_paths = {}\n",
    "        self.each_k_u_all_disjoint_paths={}\n",
    "        self.each_wk_each_k_each_user_pair_id_all_paths={}\n",
    "        self.each_wk_each_k_each_user_pair_id_paths = {}\n",
    "        self.num_of_organizations = int(config.num_of_organizations)\n",
    "        \n",
    "        self.each_k_path_path_id = {}\n",
    "        self.each_wk_each_k_user_pairs = {}\n",
    "        self.each_wk_each_k_user_pair_ids = {}\n",
    "        self.each_user_pair_all_paths = {}\n",
    "        self.each_k_weight = {}\n",
    "        self.each_k_u_weight = {}\n",
    "        self.each_wk_k_u_pair_weight = {}\n",
    "        self.each_wk_k_u_max_rate_constraint ={}\n",
    "        self.each_pair_id_paths = {}\n",
    "        self.each_scheme_each_user_pair_id_paths = {}\n",
    "        # this is becasue for each path we can have differen versios depending on distilation strategy\n",
    "        self.each_wk_k_user_pair_id_unique_path_ids = {}\n",
    "        self.each_user_organization = {}\n",
    "        self.each_wk_organizations={}\n",
    "        self.each_testing_user_organization = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.path_variables_file_path = config.flow_path_values_results\n",
    "        \n",
    "        # the value of multiplexing_value\n",
    "        self.multiplexing_value = config.multiplexing_value\n",
    "        \n",
    "        # we test the rl scheme with these many work loads\n",
    "        self.number_of_training_wks = config.number_of_training_wks\n",
    "        \n",
    "        # this is for checking which scheme is being used for path selection\n",
    "        self.running_path_selection_scheme = \"Hop\"\n",
    "        # we set how many workloads we will check the path selection scheme with\n",
    "        self.workloads_to_test = config.workloads_to_test\n",
    "        # data structures used for testing the reinforcement learnig scheme\n",
    "        self.testing_work_loads=[]\n",
    "        self.each_testing_wk_each_k_user_pairs={}\n",
    "        self.each_testing_wk_each_k_user_pair_ids ={}\n",
    "        self.each_testing_wk_k_weight = {}\n",
    "        self.each_testing_wk_organizations = {}\n",
    "        self.each_testing_wk_k_u_weight= {}\n",
    "        self.each_testing_user_organization = {}\n",
    "        self.each_testing_wk_k_u_pair_weight ={}\n",
    "        \n",
    "        self.max_edge_capacity = 0\n",
    "        self.valid_flows =[]\n",
    "        self.all_flows = []\n",
    "        self.alpha_value = 1\n",
    "        self.setting_basic_fidelity_flag = False\n",
    "        self.each_link_cost_metric = \"Hop\"\n",
    "        self.link_cost_metrics = []\n",
    "        for scheme in config.schemes:\n",
    "            if scheme in [\"EGR\",\"Hop\",\"EGRSquare\"]:\n",
    "                self.link_cost_metrics.append(scheme)\n",
    "                \n",
    "        self.cut_off_for_path_searching = int(config.cut_off_for_path_searching)\n",
    "        \n",
    "        self.candidate_paths_size_for_genetic_alg = config.candidate_paths_size_for_genetic_alg\n",
    "        #we use the object of purification for taking of purificaiton and fidelity parameters\n",
    "        self.purification = purification_object\n",
    "        self.purification.set_of_edge_level_Fth = config.set_of_edge_level_Fth\n",
    "        \n",
    "        \n",
    "        \n",
    "        # we keep track of virtual links in order to set their weights to zero in path computation\n",
    "        self.set_of_virtual_links = []\n",
    "        \n",
    "        # we need this to set the flow of each path. we need the flow Fth and the workload id and k id of a pair\n",
    "        self.each_user_pair_id_work_load = {}\n",
    "        self.each_user_pair_id_organization = {}\n",
    "        # we load the topology \n",
    "        self.load_topology()\n",
    "        \n",
    "    def split(self,x, n):\n",
    "        points = []\n",
    "        \n",
    "        \n",
    "        # If x % n == 0 then the minimum\n",
    "        # difference is 0 and all\n",
    "        # numbers are x / n\n",
    "        if (x % n == 0):\n",
    "            for i in range(n):\n",
    "    #             print(x//n, end =\" \")\n",
    "                points.append(x//n)\n",
    "        else:\n",
    "            # upto n-(x % n) the values\n",
    "            # will be x / n\n",
    "            # after that the values\n",
    "            # will be x / n + 1\n",
    "            zp = n - (x % n)\n",
    "            pp = x//n\n",
    "            for i in range(n):\n",
    "                if(i>= zp):\n",
    "    #                 print(pp + 1, end =\" \")\n",
    "                    points.append(pp + 1)\n",
    "                else:\n",
    "    #                 print(pp, end =\" \")\n",
    "                    points.append(pp)\n",
    "        return points\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "    def engineer_topology(self,target_original_network,target_link_lenght,dividing_link_metric):\n",
    "        onlyfiles = [f for f in listdir(\"data/\") if isfile(join(\"data/\", f))]\n",
    "        not_connected = 0\n",
    "        connected = 0\n",
    "        target_original_network = target_original_network.split(\"Modified\")[0]\n",
    "        for file in onlyfiles:\n",
    "            # computing distance between two nodes in graphgml data structure\n",
    "            #print(\"this is the file \",file)\n",
    "            if \"graphml\" in file and (target_original_network in file and \"Modified\" not in file):\n",
    "                G = nx.read_graphml(\"data/\"+file)\n",
    "                if nx.is_connected(G):\n",
    "                    lines = [\"Link_index\tSource\tDestination\tCapacity(EPRps)\tLength(km)\tReal/Virtual\"+\"\\n\"]\n",
    "                    randomly_selected_nodes = []\n",
    "                    each_real_link_lenght = {}\n",
    "                    each_real_link_rate = {}\n",
    "                    real_links_rates = []\n",
    "                    connected+=1\n",
    "                    latitude_values = []\n",
    "                    longitude_values = []\n",
    "                    degrees = []\n",
    "                    covered_edges = []\n",
    "                    diameter = nx.diameter(G)\n",
    "\n",
    "                    distances = []\n",
    "                    latitudes = nx.get_node_attributes(G,\"Latitude\")\n",
    "\n",
    "                    Longitudes  = nx.get_node_attributes(G,\"Longitude\")\n",
    "        #             if file ==\"Darkstrand.graphml\":\n",
    "        #                 print(\"Longitudes\",Longitudes)\n",
    "        #                 print(\"latitudes\",latitudes)\n",
    "                    counter = 0\n",
    "                    import haversine as hs\n",
    "                    nodes = []\n",
    "                    for node in G.nodes:\n",
    "\n",
    "                        nodes.append(int(node))\n",
    "                        #print(node)\n",
    "                        degrees.append(G.degree[node])\n",
    "                        try:\n",
    "                            #print(\"node %s latitudes %s Longitudes %s\"%(node, latitudes[node],Longitudes[node]))\n",
    "                            longitude_values.append(Longitudes[node])\n",
    "                            latitude_values.append(latitudes[node])\n",
    "                        except:\n",
    "                            counter+=1\n",
    "                            pass\n",
    "                    computed_distances = []\n",
    "                    for edge in G.edges:\n",
    "                        flag = True\n",
    "                        try:\n",
    "                            coords_1 = (latitudes[edge[0]],Longitudes[edge[0]])\n",
    "                        except:\n",
    "                            flag = False\n",
    "                        try:\n",
    "                            coords_2 = (latitudes[edge[1]],Longitudes[edge[1]])\n",
    "                        except:\n",
    "                            flag = False\n",
    "                        if flag:\n",
    "                            distance = hs.haversine(coords_1,coords_2)\n",
    "                            computed_distances.append(distance)\n",
    "                    edge_indx = 0\n",
    "                    node_max_id = max(nodes)+1\n",
    "                    real_edges = []\n",
    "                    all_distances = []\n",
    "                    new_repeaters_id = max(nodes)+1\n",
    "                    for edge in G.edges:\n",
    "                        flag = True\n",
    "                        try:\n",
    "                            coords_1 = (latitudes[edge[0]],Longitudes[edge[0]])\n",
    "                        except:\n",
    "                            flag = False\n",
    "                        try:\n",
    "                            coords_2 = (latitudes[edge[1]],Longitudes[edge[1]])\n",
    "                        except:\n",
    "                            flag = False\n",
    "                        if flag:\n",
    "                            distance = hs.haversine(coords_1,coords_2)\n",
    "                            if distance ==0:\n",
    "                                distance = 1\n",
    "    #                         else:\n",
    "    #                             print(\"one reasonable result\")\n",
    "                        else:\n",
    "                            distance = round(random.uniform(min(computed_distances),max(computed_distances)),3)\n",
    "                            #print(\"a randomly generated among %s %s %s\"%(min(computed_distances),max(computed_distances),distance))\n",
    "                        distance = round(distance,3)\n",
    "                        if int(distance)<=target_link_lenght:\n",
    "                            new_distance=distance\n",
    "                            c = 1\n",
    "                            etha = 10**(-0.1*0.2*new_distance)\n",
    "                            T = (new_distance*10**(-4))/25# based on simulation setup of data link layer paper\n",
    "                            if T==0:\n",
    "                                T=1 * 10**(-6)\n",
    "                            alpha = 1\n",
    "                            all_distances.append(new_distance)\n",
    "                            edge_rate = round((2*c*etha*alpha)/T,3)\n",
    "                            real_links_rates.append(edge_rate)\n",
    "                            each_real_link_rate[(int(edge[0]),int(edge[1]))] = edge_rate\n",
    "                            each_real_link_rate[(int(edge[1]),int(edge[0]))] = edge_rate\n",
    "                            each_real_link_lenght[(int(edge[0]),int(edge[1]))] = new_distance\n",
    "                            each_real_link_lenght[(int(edge[1]),int(edge[0]))] = new_distance\n",
    "                            real_edges.append((int(edge[0]),int(edge[1])))\n",
    "\n",
    "                            line = str(edge_indx)+\"\\t\"+str(edge[0])+\"\\t\"+str(edge[1])+\"\\t\"+str(edge_rate)+\"\\t\"+str(new_distance)+\"\\t\"+\"real\"+\"\\n\"\n",
    "                            edge_indx+=1\n",
    "                            lines.append(line)\n",
    "                        else:\n",
    "\n",
    "                            n = int(math.ceil(distance/target_link_lenght))\n",
    "                            list_points = split(distance, n)\n",
    "                            flag = False\n",
    "                            source = int(edge[0])\n",
    "                            end = new_repeaters_id\n",
    "                            flag = True\n",
    "\n",
    "                            for indx,point in enumerate(list_points):\n",
    "                                new_distance = point\n",
    "                                if flag:\n",
    "                                    flag = False\n",
    "                                    source = int(edge[0])\n",
    "                                    end = new_repeaters_id\n",
    "                                    new_repeaters_id+=1\n",
    "                                else:\n",
    "                                    if indx== len(list_points)-1:\n",
    "                                        source = end\n",
    "                                        end = int(edge[1])\n",
    "                                        new_repeaters_id+=1\n",
    "                                    else:\n",
    "                                        source = end\n",
    "                                        end = new_repeaters_id\n",
    "                                        new_repeaters_id+=1\n",
    "\n",
    "                                c = 1\n",
    "                                etha = 10**(-0.1*0.2*new_distance)\n",
    "                                T = (new_distance*10**(-4))/25# based on simulation setup of data link layer paper\n",
    "                                if T==0:\n",
    "                                    T=1 * 10**(-6)\n",
    "                                alpha = 1\n",
    "                                all_distances.append(new_distance)\n",
    "                                edge_rate = round((2*c*etha*alpha)/T,3)\n",
    "                                real_links_rates.append(edge_rate)\n",
    "                                each_real_link_rate[(source,end)] = edge_rate\n",
    "                                each_real_link_rate[(end,source)] = edge_rate\n",
    "                                each_real_link_lenght[(source,end)] = new_distance\n",
    "                                each_real_link_lenght[(end,source)] = new_distance\n",
    "                                real_edges.append((source,end))\n",
    "                                line = str(edge_indx)+\"\\t\"+str(source)+\"\\t\"+str(end)+\"\\t\"+str(edge_rate)+\"\\t\"+str(new_distance)+\"\\t\"+\"real\"+\"\\n\"\n",
    "                                edge_indx+=1\n",
    "                                lines.append(line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    covered_edges = []\n",
    "                    added_edges = 0\n",
    "                    node_max_id = new_repeaters_id+1\n",
    "                    for edge in real_edges:\n",
    "                        range_distance = max(all_distances) \n",
    "                        edge_distance = each_real_link_lenght[edge]\n",
    "                        paralel_links = 3\n",
    "                        for i in range(paralel_links):\n",
    "    #                         distance = round(random.uniform(edge_distance/2,3/2 *edge_distance),2)\n",
    "                            distance= edge_distance\n",
    "                            node1=edge[0]\n",
    "                            node2=edge[1]\n",
    "                            new_node1 = node_max_id\n",
    "                            added_edges+=1\n",
    "                            node_max_id = node_max_id+1\n",
    "                            covered_edges.append((node1,node2))\n",
    "                            edge_rate = max(real_links_rates)+100\n",
    "                            distance = 0\n",
    "                            line = str(edge_indx)+\"\\t\"+str(node1)+\"\\t\"+str(new_node1)+\"\\t\"+str(edge_rate)+\"\\t\"+str(distance)+\"\\t\"+\"virtual\"+\"\\n\"\n",
    "                            edge_indx+=1\n",
    "                            lines.append(line)\n",
    "\n",
    "                            #distance = round(random.uniform(min(all_distances),max(all_distances)),2)\n",
    "                            distance=edge_distance\n",
    "                            edge_rate = each_real_link_rate[edge]\n",
    "                            line = str(edge_indx)+\"\\t\"+str(new_node1)+\"\\t\"+str(node2)+\"\\t\"+str(edge_rate)+\"\\t\"+str(distance)+\"\\t\"+\"real\"+\"\\n\"\n",
    "                            edge_indx+=1\n",
    "                            lines.append(line)\n",
    "\n",
    "\n",
    "\n",
    "                         # Open the file in append & read mode ('a+')\n",
    "                    file_name = file.split(\".graphml\")[0]\n",
    "                    if os.path.exists(\"data/Modified\"+target_original_network+\".txt\"):\n",
    "                        os.remove(\"data/Modified\"+target_original_network+\".txt\")\n",
    "                    with open(\"data/Modified\"+target_original_network+\".txt\", \"a+\") as file_object:\n",
    "                        for line in lines:\n",
    "                            file_object.write(line)\n",
    "                else:\n",
    "                    not_connected+=1\n",
    "        \n",
    "        \n",
    "    def evaluate_rl_for_path_selection(self,config):\n",
    "        \"\"\"this function implements the main work flow of the reinforcement learning algorithm\"\"\"\n",
    "        # input datetime\n",
    "        dt = datetime(2018, 10, 22, 0, 0)\n",
    "        # epoch time\n",
    "        epoch_time = datetime(1970, 1, 1)\n",
    "\n",
    "        # subtract Datetime from epoch datetime\n",
    "        delta = (dt - epoch_time)\n",
    "        rl = RL(config)\n",
    "        solver = Solver()\n",
    "        self.each_link_cost_metric =\"Hop\" \n",
    "        self.set_link_weight(\"Hop\")\n",
    "#         Thread(target = rl.train, args=(config,self,)).start()\n",
    "#         time.sleep(5)\n",
    "#         Thread(target = rl.test, args=(config,self,)).start()\n",
    "        if config.training:\n",
    "            rl.train(config,self)\n",
    "        else:\n",
    "            rl.test(config,self)\n",
    "        #Thread(target = rl.test, args=(config,self,)).start()\n",
    "\n",
    "                \n",
    "    def evaluate_shortest_path_routing(self,config,link_cost_metric):\n",
    "        \"\"\"this function evaluates the entanglement generation rate using \n",
    "        shortest paths computed based on the given link cost metric\"\"\"\n",
    "        \n",
    "        \n",
    "        self.each_wk_each_k_each_user_pair_id_all_paths={}\n",
    "        self.each_wk_each_k_each_user_pair_id_paths = {}\n",
    "        solver = Solver()\n",
    "        self.each_link_cost_metric =link_cost_metric \n",
    "        self.set_link_weight(link_cost_metric)\n",
    "        workloads_with_feasible_solution = []\n",
    "        purification_schemes_string = \"\"\n",
    "        all_work_loads_egr=[]\n",
    "        for wk_idx in self.work_loads:\n",
    "            time_in_seconds = time.time()\n",
    "            self.set_paths_in_the_network(wk_idx)\n",
    "            \"\"\"we set the required EPR pairs to achieve each fidelity threshold\"\"\"\n",
    "            self.purification.set_required_EPR_pairs_for_distilation(wk_idx,self)\n",
    "            # calling the IBM CPLEX solver to solve the optimization problem\n",
    "            scheme_name = link_cost_metric+\"-based Sh-P-Edge_Fth=\"+str(self.purification.set_of_edge_level_Fth[0])\n",
    "            egr = solver.CPLEX_maximizing_EGR(wk_idx,self,config,None,0,0,0,scheme_name)\n",
    "            egr = round(egr,3)\n",
    "#             egr = solver.CPLEX_maximizing_EGR(wk_idx,self,0,0)\n",
    "#             egr = solver.CPLEX_maximizing_delta(wk_idx,self,0,0)\n",
    "#             egr = solver.CPLEX_maximizing_EGR_limited_rate_on_paths(wk_idx,self,0,0)\n",
    "            if egr >0:\n",
    "                workloads_with_feasible_solution.append(wk_idx)\n",
    "            all_work_loads_egr.append(egr)\n",
    "            #network_capacity = solver.CPLEX_swap_scheduling(wk_idx,self)\n",
    "            network_capacity= None\n",
    "#             print(\"for top %s work_load %s min_rate %s alpha_value %s metric %s number of paths %s we have egr as %s \"%\n",
    "#                   (self.topology_name,wk_idx,min_rate,self.alpha_value,link_cost_metric,self.num_of_paths,egr))\n",
    "            print(\"for top %s wk_idx %s alpha %s p2=%s eta=%s min_R= %s metric %s |U_k|=%s #paths %s egr= %s \"%\n",
    "                      (self.topology_name,wk_idx,self.alpha_value,self.purification.two_qubit_gate_fidelity,\n",
    "                       self.purification.measurement_fidelity,\n",
    "                       self.min_flow_rate,link_cost_metric,self.number_of_flows,\n",
    "                       self.num_of_paths,egr))\n",
    "            self.save_results(wk_idx,config,False,False,True,None,0,egr,0,0,time_in_seconds)\n",
    "        \n",
    "            \n",
    "\n",
    "#             with open(config.toplogy_wk_setup_feasibility_checking, 'a') as newFile:                                \n",
    "#                 newFileWriter = csv.writer(newFile)\n",
    "#                 newFileWriter.writerow([self.topology_name,\n",
    "#                 wk_idx,self.alpha_value,self.purification.two_qubit_gate_fidelity,\n",
    "#                 self.purification.measurement_fidelity,self.num_of_paths,egr,\n",
    "#                 self.target_long_link,self.repeater_placement_distance,\n",
    "#                 self.min_flow_rate,purification_schemes_string,self.number_of_flows])\n",
    "#         print(\"workloads_with_feasible_solution \",workloads_with_feasible_solution)\n",
    "#         print(\"average egr \",(sum(all_work_loads_egr)/len(all_work_loads_egr)))\n",
    "#         Edge_Fth = self.purification.set_of_edge_level_Fth[0]\n",
    "#         with open(\"results/feasibility_checkingv4.csv\", 'a') as newFile:                                \n",
    "#                 newFileWriter = csv.writer(newFile)\n",
    "#                 newFileWriter.writerow([self.topology_name,self.alpha_value,\n",
    "#                 self.purification.two_qubit_gate_fidelity,\n",
    "#                 self.purification.measurement_fidelity\n",
    "#                 ,self.number_of_flows,Edge_Fth,len(workloads_with_feasible_solution),sum(all_work_loads_egr)/len(all_work_loads_egr)])\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_each_user_all_paths(self,wk_idx,rl_testing_flag):\n",
    "        \"\"\"this function will set all the paths of each user pair for the given work load\"\"\"\n",
    "        \n",
    "#         if rl_testing_flag:\n",
    "#             print(\"*********************** getting all user pairs all paths of workload %s ***************\"%(wk_idx))\n",
    "        self.each_user_pair_id_all_paths = {}\n",
    "        if rl_testing_flag:\n",
    "            \"\"\"this part is for the user pairs that exist only in the testing workload but are not in trainign workload\"\"\"\n",
    "#             for k, user_pair_ids in self.each_testing_wk_each_k_user_pair_ids[wk_idx].items():\n",
    "            for k, user_pair_ids in self.each_wk_each_k_user_pair_ids[wk_idx].items():\n",
    "                #print(\"we have these user pairs %s in work load %s \"%(user_pair_ids,wk_idx))\n",
    "                for user_pair_id in user_pair_ids:\n",
    "                    user_pair = self.each_wk_k_id_pair[wk_idx][k][user_pair_id]\n",
    "                    self.each_user_pair_id_all_paths[user_pair_id]= []\n",
    "                    for path_id in self.each_pair_id_paths[user_pair_id]:\n",
    "                        try:\n",
    "                            self.each_user_pair_id_all_paths[user_pair_id].append(path_id)\n",
    "                        except:\n",
    "                            self.each_user_pair_id_all_paths[user_pair_id] = [path_id]\n",
    "        else:\n",
    "            for k, user_pair_ids in self.each_wk_each_k_user_pair_ids[wk_idx].items():\n",
    "                for user_pair_id in user_pair_ids:\n",
    "                    user_pair = self.each_wk_k_id_pair[wk_idx][k][user_pair_id]\n",
    "                    self.each_user_pair_id_all_paths[user_pair_id]= []\n",
    "                    for path_id in self.each_pair_id_paths[user_pair_id]:\n",
    "                        try:\n",
    "                            self.each_user_pair_id_all_paths[user_pair_id].append(path_id)\n",
    "                        except:\n",
    "                            self.each_user_pair_id_all_paths[user_pair_id] = [path_id]\n",
    "                        \n",
    "\n",
    "            \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    def set_paths_from_chromosome(self,wk_idx,chromosome):\n",
    "        \"\"\"this function uses the information in the chromosome \n",
    "        to set the paths to the data structure that will be used by solver\"\"\"\n",
    "        \n",
    "        #chromosome =  [18250, 18279, 18276, 18311, 18387, 18354, 18433, 18461, 18432, 18556, 18486, 18534, 18577, 18639, 18631, 18730, 18717, 18694, 18892, 18815, 18807, 18944, 18950, 18941, 19046, 19033, 18976, 19118, 19079, 19077, 19251, 19252, 19193, 19300, 19306, 19357, 19421, 19437, 19428, 19605, 19597, 19608, 19742, 19718, 19692, 19827, 19828, 19877, 20024, 20069, 20057, 20115, 20134, 20106, 20263, 20292, 20342, 20365, 20360, 20454, 20533, 20482, 20532, 20687, 20646, 20761, 20833, 20802, 20821, 20860, 20848, 20920, 21021, 21016, 21049, 21142, 21116, 21106, 21181, 21189, 21195, 21284, 21252, 21233, 21348, 21312, 21320, 21455, 21461, 21440, 21680, 21527, 21613, 21772, 21740, 21730, 21935, 21876, 21879, 21952, 21958, 22007, 22116, 22071, 22168, 22208, 22244, 22262, 22382, 22444, 22365, 22515, 22563, 22498, 22780, 22799, 22677, 22801, 22843, 22815, 22915, 22955, 22888, 23027, 22985, 23006, 23068, 23059, 23134, 23299, 23307, 23315, 23420, 23410, 23467, 23529, 23540, 23506, 23636, 23713, 23725, 23853, 23788, 23850, 23922, 24043, 23918, 24191, 24205, 24175, 24227, 24253, 24282, 24371, 24331, 24348, 24416, 24464, 24440, 24626, 24588, 24634, 24750, 24684, 24755, 24823, 24846, 24806, 24873, 24986, 24985, 25076, 25097, 25020, 25336, 25332, 25201, 25385, 25381, 25380, 25495, 25456, 25527, 25601, 25573, 25597, 25644, 25636, 25658, 25789, 25788, 25771, 25807, 25956, 25893, 26129, 26029, 26090, 26194, 26219, 26193, 26408, 26418, 26322, 26483, 26480, 26536, 26574, 26596, 26677, 26735, 26782, 26783, 26823, 26859, 26805, 26913, 26940, 26946, 27029, 27040, 27086, 27195, 27210, 27275, 27421, 27381, 27433, 27481, 27490, 27497, 27582, 27537, 27596, 27707, 27764, 27760, 27787, 27801, 27802, 27924, 28026, 28004, 28167, 28057, 28126, 28251, 28364, 28312, 28470, 28473, 28459, 28500, 28556, 28513, 28612, 28778, 28774, 28872, 28817, 28893, 28957, 28922, 28972, 29121, 29098, 29204, 29251, 29289, 29286, 29421, 29387, 29375, 29442, 29452, 29503, 29505, 29540, 29507, 29652, 29658, 29656, 29817, 29852, 29850, 30092, 30026, 30030, 30116, 30130, 30117, 30222, 30255, 30239, 30383, 30387, 30382, 30582, 30606, 30448, 30772, 30644, 30782, 30870, 30842, 30875, 30979, 30933, 30961, 31098, 31050, 31086, 31290, 31294, 31247, 31374, 31382, 31341, 31402, 31397, 31428, 31513, 31689, 31510, 31783, 31775, 31715, 31903, 31852, 31916, 31986, 31996, 31937, 32092, 32094, 32089, 32173, 32150, 32140, 32185, 32190, 32202, 32381, 32355, 32282, 32472, 32446, 32475, 32560, 32521, 32573, 32652, 32672, 32607, 32794, 32809, 32731, 32870, 32862, 32830, 32949, 32940, 33033, 33157, 33199, 33077, 33299, 33293, 33259, 33326, 33362, 33381, 33440, 33496, 33428, 33577, 33610, 33590, 33730, 33706, 33651, 33772, 33881, 33797, 33957, 33974, 33978, 34082, 34109, 34157, 34276, 34254, 34257, 34324, 34330, 34383, 34444, 34442, 34505, 34655, 34593, 34608, 34785, 34755, 34783, 34867, 34873, 34917, 35004, 35058, 34988, 35079, 35189, 35203, 35294, 35283, 35329, 35397, 35397, 35456, 35603, 35618, 35498, 35738, 35733, 35689, 35880, 35896, 35887, 35962, 36027, 36017, 36106, 36075, 36072, 36209, 36212, 36168, 36483, 36405, 36413, 36574, 36576, 36550, 36731, 36721, 36714, 36808, 36777, 36757]\n",
    "        \n",
    "        \n",
    "        \"\"\"we set the required EPR pairs to achieve each fidelity threshold\"\"\"\n",
    "        self.purification.set_required_EPR_pairs_for_distilation(wk_idx,self)\n",
    "        \n",
    "        \n",
    "        def check_any_version_of_the_path_has_been_added(path_id,path_ids,path_versions):\n",
    "            for path in path_versions:\n",
    "                if path in path_ids:\n",
    "                    return False\n",
    "            return True\n",
    "        path_indx = 0\n",
    "        for k in self.each_wk_organizations[wk_idx]:\n",
    "            for user_pair_id in self.each_wk_each_k_user_pair_ids[wk_idx][k]:\n",
    "                path_ids = chromosome[path_indx:path_indx+self.num_of_paths]\n",
    "                path_ids = list(set(path_ids))\n",
    "\n",
    "#                     print(\"for wk %s k %s u %s we have %s paths \"%(wk_idx,k,user_pair_id,len(path_ids)))\n",
    "#                     for p in path_ids:\n",
    "#                         print(\"versions: \",p,self.purification.each_path_version_numbers[p])\n",
    "#                         print(\"g_value:  \",p,self.purification.each_path_edge_level_g[p])\n",
    "                # all of the 20 lines of code is to avoid having a duplicate path even a path with duplicate distilation strategy\n",
    "                # we pick th path with highest value of g function for edge-level distilation \n",
    "                new_path_ids = []\n",
    "                each_path_versions ={}\n",
    "                for path_id in path_ids:\n",
    "                    each_path_versions[path_id] = [path_id]\n",
    "                    if path_id!=-1:\n",
    "                        for sec_v_path_id in path_ids:\n",
    "                            if path_id!= sec_v_path_id:\n",
    "                                if sec_v_path_id in self.purification.each_path_version_numbers[path_id]:\n",
    "                                    try:\n",
    "                                        each_path_versions[path_id].append(sec_v_path_id)\n",
    "                                    except:\n",
    "                                        each_path_versions[path_id]=[sec_v_path_id]\n",
    "                for path_id,version_path_ids in each_path_versions.items():\n",
    "                    edge_g_value = self.purification.each_path_edge_level_g[path_id]\n",
    "                    edge_level_g_values = []\n",
    "                    for path_id_v in version_path_ids:\n",
    "                        edge_level_g_values.append(self.purification.each_path_edge_level_g[path_id_v])\n",
    "                    if edge_g_value >= max(edge_level_g_values):\n",
    "                        if path_id not in new_path_ids:\n",
    "                            if check_any_version_of_the_path_has_been_added(path_id,new_path_ids,self.purification.each_path_version_numbers[path_id]):\n",
    "                                if self.purification.check_path_distilability(path_id):\n",
    "                                    new_path_ids.append(path_id)\n",
    "\n",
    "                path_ids = new_path_ids\n",
    "                path_indx = path_indx+self.num_of_paths\n",
    "                k = self.each_user_organization[user_pair_id]\n",
    "#                     for path_id in path_ids:\n",
    "#                         for path_id2 in path_ids:\n",
    "#                             if path_id1!=path_id2 and both_same():\n",
    "#                     print(\"for wk %s k %s u %s we are adding %s paths \"%(wk_idx,k,user_pair_id,len(path_ids)))\n",
    "                try:\n",
    "                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = path_ids\n",
    "                except:\n",
    "                    try:\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = path_ids\n",
    "                    except:\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx]={}\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id]=path_ids\n",
    "        \n",
    "        \n",
    "        \n",
    "    def save_rl_results_for_genetic_initialization(self,config,wk_idx,epoch_number,egr):\n",
    "        \"\"\"we save the paths that the rl suggests for this work load and use ot later \n",
    "        to initialize the genetic algorthm first population\"\"\"\n",
    "        with open(self.toplogy_wk_rl_for_initialization_ga_result_file, 'a') as newFile:                                \n",
    "                newFileWriter = csv.writer(newFile)\n",
    "                for k in self.each_wk_organizations[wk_idx]:\n",
    "                    for u in self.each_wk_each_k_user_pair_ids[wk_idx][k]: \n",
    "                        for p in self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][u]:\n",
    "                            newFileWriter.writerow([self.topology_name,wk_idx,self.num_of_paths,\n",
    "                            0,self.q_value,\n",
    "                            self.each_link_cost_metric,self.number_of_flows,config.number_of_training_wks,\n",
    "                                egr,epoch_number,\n",
    "                                config.cut_off_for_path_searching,k,u,p,self.set_of_paths[p],\n",
    "                                \n",
    "                                self.alpha_value,\n",
    "               \n",
    "                                        config.rl_batch_size,config.initial_learning_rate,\n",
    "                                       config.learning_rate_decay_rate,\n",
    "                                       config.moving_average_decay,\n",
    "                                        config.learning_rate_decay_step_multiplier,\n",
    "                                        config.learning_rate_decay_step,\n",
    "                                       config.entropy_weight,config.optimizer,config.scale,\n",
    "                                        config.max_step,config.number_of_training_wks,\n",
    "                                        self.purification.two_qubit_gate_fidelity,\n",
    "                                        self.purification.measurement_fidelity,\n",
    "                                       len(config.set_of_edge_level_Fth),config.num_of_organizations])\n",
    "        \n",
    "        \n",
    "    def save_results(self,wk_idx,config,genetic_alg_flag,rl_flag,shortest_path_flag,genetic_alg,\n",
    "                     runs_of_algorithm,egr,optimal_egr,run_number,time_in_seconds):\n",
    "        if config.optimization_problem_with_minimum_rate:\n",
    "            minimum_rate_constraint = \"True\"\n",
    "        else:\n",
    "            minimum_rate_constraint = \"False\"\n",
    "            \n",
    "        if config.dynamic_policy_flag:\n",
    "            dynamic_static = \"True\"\n",
    "        else:\n",
    "            dynamic_static = \"False\"\n",
    "            \n",
    "        if self.optimization_problem_with_maximum_rate:\n",
    "            max_flow_rate_flag = \"True\"\n",
    "        else:\n",
    "            max_flow_rate_flag=\"False\"\n",
    "        purification_schemes_string = \"\"\n",
    "        for pur_scheme in self.purification.allowed_purification_schemes:\n",
    "            if purification_schemes_string:\n",
    "                purification_schemes_string = purification_schemes_string+\",\"+pur_scheme\n",
    "            else:\n",
    "                purification_schemes_string = pur_scheme\n",
    "        if genetic_alg_flag:\n",
    "            with open(self.toplogy_wk_scheme_result, 'a') as newFile:                                \n",
    "                newFileWriter = csv.writer(newFile)\n",
    "                newFileWriter.writerow([self.topology_name,wk_idx,self.alpha_value,self.num_of_paths,\n",
    "                0,purification_schemes_string,self.q_value,\n",
    "                \"Genetic\",self.number_of_flows,genetic_alg.elit_pop_size,\n",
    "                                        genetic_alg.crossover_p,genetic_alg.mutation_p,runs_of_algorithm,\n",
    "                                        egr,genetic_alg.number_of_chromosomes,run_number,\n",
    "                                        config.genetic_algorithm_random_initial_population,\n",
    "                                        config.ga_elit_pop_update_step,config.ga_crossover_mutation_update_step,\n",
    "                                        config.cut_off_for_path_searching,config.multi_point_mutation_value,\n",
    "                                        config.multi_point_crossover_value,\n",
    "                                        config.ga_crossover_mutation_multi_point_update_step,\n",
    "                                        config.genetic_algorithm_initial_population_rl_epoch_number,\n",
    "                                       config.number_of_training_wks,\n",
    "                                        config.genetic_algorithm_initial_population,\n",
    "                                        self.purification.two_qubit_gate_fidelity,\n",
    "                                        self.purification.measurement_fidelity,time_in_seconds,\n",
    "                                        self.candidate_paths_size_for_genetic_alg,\n",
    "                                        len(config.set_of_edge_level_Fth),config.num_of_organizations,\n",
    "                                       minimum_rate_constraint,dynamic_static,\n",
    "                                       max_flow_rate_flag,self.max_flow_rate])\n",
    "        elif shortest_path_flag:\n",
    "            Edge_Fth = self.purification.set_of_edge_level_Fth[0]\n",
    "            scheme = self.each_link_cost_metric+\"-Edge_Fth=\"+str(Edge_Fth)\n",
    "            with open(self.toplogy_wk_scheme_result, 'a') as newFile:                                \n",
    "                newFileWriter = csv.writer(newFile)\n",
    "                newFileWriter.writerow([self.topology_name,wk_idx,self.alpha_value,self.num_of_paths,\n",
    "                0,purification_schemes_string,self.q_value,\n",
    "                scheme,self.number_of_flows,0,0,\n",
    "                                        0,1,\n",
    "                                        egr,0,run_number,\n",
    "                                        0,\n",
    "                                        0,0,\n",
    "                                        config.cut_off_for_path_searching,0,\n",
    "                                        0,0,self.purification.two_qubit_gate_fidelity,\n",
    "                                        self.purification.measurement_fidelity,\n",
    "                                        time_in_seconds,config.num_of_organizations,\n",
    "                                       minimum_rate_constraint,\n",
    "                                       max_flow_rate_flag,self.max_flow_rate])\n",
    "        elif rl_flag:\n",
    "            with open(self.toplogy_wk_scheme_result, 'a') as newFile:                                \n",
    "                newFileWriter = csv.writer(newFile)\n",
    "                newFileWriter.writerow([self.topology_name,wk_idx,\n",
    "                                        self.alpha_value,self.num_of_paths,\n",
    "                0,purification_schemes_string,self.q_value,\n",
    "                \"RL\",self.number_of_flows,0,0,\n",
    "                                        0,runs_of_algorithm,\n",
    "                                        egr,0,run_number,\n",
    "                                        0,\n",
    "                                        0,0,\n",
    "                                        config.cut_off_for_path_searching,0,\n",
    "                                        0,0,config.rl_batch_size,config.initial_learning_rate,\n",
    "                                       config.learning_rate_decay_rate,\n",
    "                                       config.moving_average_decay,\n",
    "                                        config.learning_rate_decay_step_multiplier,\n",
    "                                        config.learning_rate_decay_step,\n",
    "                                       config.entropy_weight,config.optimizer,config.scale,\n",
    "                                        config.max_step,config.number_of_training_wks,\n",
    "                                        self.purification.two_qubit_gate_fidelity,\n",
    "                                        self.purification.measurement_fidelity,time_in_seconds,\n",
    "                                       len(config.set_of_edge_level_Fth),\n",
    "                                       config.num_of_organizations,\n",
    "                                       minimum_rate_constraint,\n",
    "                                       max_flow_rate_flag,self.max_flow_rate])\n",
    "        \n",
    "#                      \n",
    "\n",
    "    def get_flows_minimum_rate(self,wk_idx):\n",
    "        self.each_flow_minimum_rate = {}\n",
    "        with open(self.each_flow_minimum_rate_value_file, \"r\") as f:\n",
    "            reader = csv.reader( (line.replace('\\0','') for line in f) )\n",
    "            for line in reader:  #flow,flow_pair,rate    \n",
    "                wk_idx = int(line[0])\n",
    "                k = int(line[1])\n",
    "                flow_id= int(line[2])\n",
    "                flow_pair = line[3]\n",
    "                flow_pair = flow_pair.replace(\"(\",\"\")\n",
    "                flow_pair = flow_pair.replace(\")\",\"\")\n",
    "                flow_pair = flow_pair.split(\",\")\n",
    "                flow_pair_src = flow_pair[0]\n",
    "                flow_pair_dst = flow_pair[1]\n",
    "                flow_pair_src = int(flow_pair_src)\n",
    "                flow_pair_dst = int(flow_pair_dst)\n",
    "                flow = (flow_pair_src,flow_pair_dst)\n",
    "                minimum_rate = max(0,float(line[4])-4)\n",
    "#                 print(\"for wk %s k %s flow %s from file %s  id %s min R %s \"%(wk_idx,k,flow, flow_id,flow_pair,minimum_rate))\n",
    "                self.each_flow_minimum_rate[wk_idx,k,flow] = minimum_rate\n",
    "#                 time.sleep(1)\n",
    "            \n",
    "                \n",
    "    def evaluate_genetic_algorithm_for_path_selection(self,config):\n",
    "        \"\"\"this function implements the main work flow of the genetic algorithm\"\"\"\n",
    "        solver = Solver()\n",
    "        self.each_link_cost_metric =\"Hop\" \n",
    "        self.set_link_weight(\"Hop\")\n",
    "        runs_of_genetic_algorithm = 0\n",
    "        genetic_alg = Genetic_algorithm(config)\n",
    "        genetic_alg.multi_point_crossover_value = min(config.multi_point_crossover_value,\n",
    "                                                      config.num_of_organizations*self.num_of_paths*self.number_of_flows)\n",
    "        genetic_alg.multi_point_mutation_value = min(config.multi_point_mutation_value,\n",
    "                                                     config.num_of_organizations*self.num_of_paths*self.number_of_flows)\n",
    "        genetic_alg.static_multi_point_crossover_value = min(config.static_multi_point_crossover_value,\n",
    "                                                             config.num_of_organizations*self.num_of_paths*self.number_of_flows)\n",
    "        genetic_alg.static_multi_point_mutation_value = min(config.static_multi_point_mutation_value,\n",
    "                                                            config.num_of_organizations*self.num_of_paths*self.number_of_flows)\n",
    "        max_runs_of_genetic_algorithm = 1 # maximum number of populations during genetic algorithm search\n",
    "#         for wk_idx in self.work_loads:# Each work load includes a different set of user pairs in the network\n",
    "        \n",
    "        machine_name = \"Unknown!\"\n",
    "        machine_name = config.toplogy_wk_scheme_result_file\n",
    "        try:\n",
    "            machine_name = str(machine_name.split(\"results/\")[1].split(\"with\")[0])\n",
    "        except:\n",
    "            machine_name = \"Unknown!\"\n",
    "            \n",
    "        if config.optimization_problem_with_minimum_rate:\n",
    "            minimum_rate_constraint = \"True\"\n",
    "        else:\n",
    "            minimum_rate_constraint = \"False\"\n",
    "        \n",
    "        for wk_idx in range(config.wkidx_min_value,config.wkidx_max_value):\n",
    "            if wk_idx in config.workloads_with_feasible_solution:\n",
    "                print(\"for wk indx \",wk_idx)\n",
    "                if config.get_flows_minimum_rate_flag:\n",
    "                    each_flow_minimum_rate = {}\n",
    "                    self.get_flows_minimum_rate(wk_idx)\n",
    "                \n",
    "                \n",
    "                for elit_pop_size in genetic_alg.elit_pop_sizes:# percentage of top chromosomes that we generate next population\n",
    "                    genetic_alg.elit_pop_size = elit_pop_size\n",
    "\n",
    "                    for cross_over_value in genetic_alg.cross_over_values:# probability of applying crossover\n",
    "                        genetic_alg.crossover_p = cross_over_value\n",
    "                        for mutation_op_value in genetic_alg.mutation_op_values:# probability of applying mutation\n",
    "                            genetic_alg.mutation_p =mutation_op_value \n",
    "                            for population_size in genetic_alg.population_sizes:\n",
    "                                print(\"for crossover %s mutation %s population %s \"%(cross_over_value,mutation_op_value,population_size))\n",
    "                                genetic_alg.number_of_chromosomes = population_size\n",
    "                                \"\"\"we set the set of all paths (all n shortest paths using different link cost metrics)\"\"\"\n",
    "                                self.get_each_user_all_paths(wk_idx,False)\n",
    "                                for i in range(config.runs_of_genetic_algorithm):\n",
    "                                    # we print the path ids for each user pair id\n",
    "                                    self.each_user_organization = {}\n",
    "                                    \n",
    "                                    genetic_alg.generate_chromosomes(wk_idx,i,self,config)\n",
    "                                    max_fitness_value = 0\n",
    "                                    best_chromosome = \"\"\n",
    "                                    genetic_algorithm_running_flag = True\n",
    "                                    runs_of_genetic_algorithm = 0\n",
    "                                    while(runs_of_genetic_algorithm < genetic_alg.max_runs_of_genetic_algorithm):\n",
    "                                        genetic_alg.each_fitness_chromosomes = {}\n",
    "                                        chromosome_id = 0\n",
    "                                        # input datetime\n",
    "                                        fitness_values = []\n",
    "                                        time_in_seconds = time.time()\n",
    "                                        for chromosome in genetic_alg.chromosomes:\n",
    "                                            self.set_paths_from_chromosome(wk_idx,chromosome)\n",
    "                                            fitness_value  = solver.CPLEX_maximizing_EGR(wk_idx,self,config,genetic_alg,-1,-1,chromosome_id,\"Genetic\")\n",
    "                                            fitness_value = round(fitness_value,3)\n",
    "                                            if fitness_value not in fitness_values:\n",
    "                                                fitness_values.append(fitness_value)\n",
    "                                            try:\n",
    "                                                genetic_alg.each_fitness_chromosomes[fitness_value].append(chromosome)\n",
    "                                            except:\n",
    "                                                genetic_alg.each_fitness_chromosomes[fitness_value] = [chromosome]\n",
    "\n",
    "                                            # we save the max egr at this point of genetic algorithm\n",
    "                                            self.save_results(wk_idx,config,True,False,False,genetic_alg,runs_of_genetic_algorithm,fitness_value,0,i,time_in_seconds)\n",
    "                                            genetic_alg.update_operation_probabilities(runs_of_genetic_algorithm,config)\n",
    "                                            chromosome_id+=1\n",
    "\n",
    "                                            if runs_of_genetic_algorithm%20==0:\n",
    "\n",
    "                                                print(\"wk %s run %s |U_k| %s num_k %s # P %s dyn/stc %s |DS| %s |P_can| %s Min_R %s mach %s chro %s th from %s egr %s step %s / %s\"\n",
    "                                                  %(wk_idx,i,self.number_of_flows,config.num_of_organizations,\n",
    "                                                    self.num_of_paths,config.dynamic_policy_flag,len(config.set_of_edge_level_Fth),config.candidate_paths_size_for_genetic_alg,\n",
    "                                                    minimum_rate_constraint,machine_name,chromosome_id,len(genetic_alg.chromosomes),fitness_value,\n",
    "                                                    runs_of_genetic_algorithm,genetic_alg.max_runs_of_genetic_algorithm))\n",
    "                                                #print(\"******************************* one round of genetic algorithm remained %s*******************\",genetic_alg.max_runs_of_genetic_algorithm-runs_of_genetic_algorithm)\n",
    "                                        max_fitness = max(fitness_values)\n",
    "                                        best_chromosome = genetic_alg.each_fitness_chromosomes[max_fitness][0]\n",
    "                                        \n",
    "                                        self.set_paths_from_chromosome(wk_idx,best_chromosome)\n",
    "                                        fitness_value  = solver.CPLEX_maximizing_EGR(wk_idx,self,config,genetic_alg,runs_of_genetic_algorithm,i,0,\"Genetic\")\n",
    "#                                         print(\"*** our best chromosome at step %s was %s with fitness %s ****\"%(runs_of_genetic_algorithm,best_chromosome,fitness_value))\n",
    "#                                         print(\"best chromosome %s \"%(best_chromosome))\n",
    "#                                         time.sleep(1)\n",
    "#                                         print(\"best chromosome \",best_chromosome)\n",
    "#                                         time.sleep(10)\n",
    "                                        genetic_alg.population_gen_op()\n",
    "                                        print(\"wk %s run %s |U_k| %s num_k %s # P %s dyn/stc %s |DS| %s |P_can| %s mach %s Pop %s step %s / %s\"\n",
    "                                                  %(wk_idx,i,self.number_of_flows,config.num_of_organizations,\n",
    "                                                    self.num_of_paths,config.dynamic_policy_flag,len(config.set_of_edge_level_Fth),\n",
    "                                                    config.candidate_paths_size_for_genetic_alg,\n",
    "                                                    machine_name,len(genetic_alg.chromosomes),\n",
    "                                                    runs_of_genetic_algorithm,genetic_alg.max_runs_of_genetic_algorithm))\n",
    "                                        runs_of_genetic_algorithm+=1\n",
    "    \n",
    "    \n",
    "   \n",
    "                                    \n",
    "    def load_topology(self):\n",
    "        self.set_E=[]\n",
    "        self.each_edge_capacity={}\n",
    "        self.nodes = []\n",
    "        self.purification.each_edge_fidelity = {}\n",
    "        self.link_idx_to_sd = {}\n",
    "        self.link_sd_to_idx = {}\n",
    "        edge_counter = 0\n",
    "        self.g = nx.Graph()\n",
    "        print('[*] Loading topology...', self.topology_file)\n",
    "        try:\n",
    "            f = open(self.topology_file+\".txt\", 'r')\n",
    "        except:\n",
    "            f = open(self.topology_file, 'r')\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            edge_counter+=1\n",
    "            line = line.strip()\n",
    "            link = line.split('\\t')\n",
    "            #print(line,link)\n",
    "            real_virtual = \"real\"\n",
    "            try:\n",
    "                i, s, d,  c,l,real_virtual = link\n",
    "            except:\n",
    "                i, s, d,  c,l = link\n",
    "            if real_virtual ==\"virtual\":\n",
    "                self.set_of_virtual_links.append((int(s),int(d)))\n",
    "                self.set_of_virtual_links.append((int(d),int(s)))\n",
    "                \n",
    "                self.purification.virtual_links.append((int(s),int(d)))\n",
    "                self.purification.virtual_links.append((int(d),int(s)))\n",
    "                \n",
    "            if int(s) not in self.nodes:\n",
    "                self.nodes.append(int(s))\n",
    "            if int(d) not in self.nodes:\n",
    "                self.nodes.append(int(d))\n",
    "            self.set_E.append((int(s),int(d)))\n",
    "            edge_capacity = float(c)\n",
    "#             edge_capacity = 10000\n",
    "            if edge_capacity >self.max_edge_capacity:\n",
    "                self.max_edge_capacity = edge_capacity\n",
    "                self.purification.upper_bound_on_distilation = self.max_edge_capacity+100\n",
    "            \n",
    "            self.each_edge_distance[(int(s),int(d))] = float(l)\n",
    "            self.each_edge_distance[(int(d),int(s))] = float(l)\n",
    "            self.each_edge_capacity[(int(s),int(d))] = edge_capacity\n",
    "            self.each_edge_capacity[(int(d),int(s))] = edge_capacity\n",
    "            if (int(s),int(d)) in self.g.edges:\n",
    "                print(\"duplicate  edge \",int(s),int(d))\n",
    "            if real_virtual ==\"virtual\":\n",
    "                self.g.add_edge(int(s),int(d),capacity=edge_capacity,weight=1)\n",
    "                self.g.add_edge(int(d),int(s),capacity=edge_capacity,weight=1)\n",
    "            else:\n",
    "                self.g.add_edge(int(s),int(d),capacity=edge_capacity,weight=1)\n",
    "                self.g.add_edge(int(d),int(s),capacity=edge_capacity,weight=1)\n",
    "        f.close()\n",
    "        print(\"*********            # edges %s edge_counter %s self.set_E %s ************** \"%(len(self.g.edges),edge_counter,len(self.set_E)))\n",
    "\n",
    "        \n",
    "        \n",
    "    def set_nodes_q_value(self):\n",
    "        for node in self.nodes:\n",
    "            self.each_node_q_value[node] = self.q_value\n",
    "    def find_longest_path(self,source,destination):\n",
    "        # Get the longest path from node source to node destination\n",
    "        longest_path = max(nx.all_simple_paths(self.g, source, destination), key=lambda x: len(x))\n",
    "        return longest_path\n",
    "    \n",
    "    def get_path_info(self):\n",
    "        self.all_user_pairs_across_wks = []\n",
    "        self.each_pair_id_paths = {}\n",
    "        self.each_scheme_each_user_pair_id_paths = {}\n",
    "        set_of_all_paths = []\n",
    "        self.path_counter_id = 0\n",
    "      \n",
    "#         print(\"self.each_wk_organizations\",self.each_wk_organizations)\n",
    "#         for wk,ks in self.each_testing_wk_organizations.items():\n",
    "#             for k in ks:\n",
    "#                 try:\n",
    "#                     for u in self.each_testing_wk_each_k_user_pairs[wk][k]:\n",
    "#                         if u not in self.all_user_pairs_across_wks:\n",
    "#                             self.all_user_pairs_across_wks.append(u)\n",
    "#                 except:\n",
    "#                     pass\n",
    "                \n",
    "#         for wk,ks in self.each_wk_organizations.items():\n",
    "#             for k in ks:\n",
    "# #                 print(\"these are ks \",ks)\n",
    "# #                 print(\"and these are what we have set pairs for them \",self.each_wk_each_k_user_pairs[wk])\n",
    "#                 for u in self.each_wk_each_k_user_pairs[wk][k]:\n",
    "#                     if u not in self.all_user_pairs_across_wks:\n",
    "#                         #if self.running_path_selection_scheme==\"RL\":\n",
    "#                         self.all_user_pairs_across_wks.append(u)\n",
    "\n",
    "        for wk_idx,ks in self.each_wk_organizations.items():\n",
    "            print(\"extracting paths info for wk %s out of %s \"%(wk_idx,len(list(self.each_wk_organizations.keys()))))\n",
    "            for k in ks:\n",
    "                for user_pair in self.each_wk_each_k_user_pairs[wk_idx][k]:\n",
    "                #for user_pair in self.all_user_pairs_across_wks:\n",
    "                    having_atleast_one_path_flag = False\n",
    "                    set_of_all_paths = []\n",
    "                    for link_cost_metric in [\"Hop\",\"EGR\",\"EGRSquare\"]:\n",
    "        #             for link_cost_metric in [\"Hop\"]:#just for now to fix the isuue with iniializing the genetic algorthm\n",
    "                        self.each_link_cost_metric =link_cost_metric \n",
    "                        self.set_link_weight(link_cost_metric)\n",
    "        #                 k = self.each_user_organization[user_pair]\n",
    "                        user_pair_id = self.each_wk_k_pair_id[wk_idx][k][user_pair]\n",
    "                        paths = self.get_paths_between_user_pairs(user_pair)\n",
    "                        #random.shuffle(paths)\n",
    "                        selected_path_for_this_scheme = []\n",
    "                        path_flag = False\n",
    "                        for path in paths:\n",
    "                            node_indx = 0\n",
    "                            path_edges = []\n",
    "                            for node_indx in range(len(path)-1):                \n",
    "                                path_edges.append((path[node_indx],path[node_indx+1]))\n",
    "                                node_indx+=1\n",
    "\n",
    "                            if path_edges not in set_of_all_paths:\n",
    "                                set_of_all_paths.append(path_edges)\n",
    "                                path_fidelity  = self.purification.get_fidelity(path_edges,self.set_of_virtual_links)\n",
    "                                #if path_fidelity>0.50:\n",
    "            #                     wk_idx=self.each_user_pair_id_work_load[user_pair_id]\n",
    "            #                     k = self.each_user_pair_id_organization[user_pair_id]\n",
    "\n",
    "                                path_flag= True\n",
    "                                having_atleast_one_path_flag = True\n",
    "                                set_of_all_paths.append(path_edges)\n",
    "                                set_of_different_version_of_a_path = []\n",
    "                                \n",
    "                                for edge_level_Fth in self.purification.set_of_edge_level_Fth:\n",
    "                                    if edge_level_Fth==0.992:\n",
    "                                        try:\n",
    "                                            self.each_wk_k_user_pair_id_unique_path_ids[wk_idx,k,user_pair_id,link_cost_metric].append(self.path_counter_id)\n",
    "                                        except:\n",
    "                                            self.each_wk_k_user_pair_id_unique_path_ids[wk_idx,k,user_pair_id,link_cost_metric] = [self.path_counter_id]\n",
    "                                        \n",
    "                                    self.purification.each_path_edge_level_Fth[self.path_counter_id]=edge_level_Fth\n",
    "\n",
    "                                    self.set_each_path_length(self.path_counter_id,path_edges)\n",
    "                                    self.set_of_paths[self.path_counter_id] = path_edges\n",
    "                                    # we set what is the fidelity threshold of the flow that uses this path\n",
    "                                    self.purification.each_path_basic_fidelity[self.path_counter_id]= round(path_fidelity,3)\n",
    "                                    #print(\"these are the user pair ids in wk %s k %s user_pair %s user_pair_id %s: %s\"%(wk_idx,k,user_pair,user_pair_id,self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]))\n",
    "    #                                 print(\"we set Fth %s for wk %s k %s path %s from flow %s \"%(self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id],wk_idx,k,self.path_counter_id,user_pair_id))\n",
    "                                    self.purification.each_path_flow_target_fidelity[self.path_counter_id] = self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id]\n",
    "                                    if self.path_counter_id not in selected_path_for_this_scheme:\n",
    "                                        selected_path_for_this_scheme.append(self.path_counter_id)\n",
    "\n",
    "    #                                 print(\"we set path wk %s k %s flow %s with id %s path id %s \"%(wk_idx,k,user_pair,user_pair_id,self.path_counter_id))\n",
    "                                    try:\n",
    "                                        self.each_pair_id_paths[user_pair_id].append(self.path_counter_id)\n",
    "                                    except:\n",
    "                                        self.each_pair_id_paths[user_pair_id] = [self.path_counter_id]\n",
    "                                    \"\"\"here we set what is the purification scheme for each path id.\n",
    "                                    for each path, there could multiple versions\n",
    "                                    depends on the number of purification schemes\n",
    "                                    e.g, end-level,edge-level, multi-hop level\"\"\"\n",
    "                                    #self.purification.each_path_id_purificaiton_scheme[self.path_counter_id] = pur_scheme\n",
    "                                    set_of_different_version_of_a_path.append(self.path_counter_id)\n",
    "                                    self.path_counter_id+=1  \n",
    "                                \n",
    "                                #we put different versions of a path in order to prevent using a path with differernt purification schemes at the same time\n",
    "                                for path_id in set_of_different_version_of_a_path:\n",
    "                                    self.purification.each_path_version_numbers[path_id] = set_of_different_version_of_a_path\n",
    "\n",
    "#                         print(\"for scheme %s wk %s k %s u %s  pair %s we have min path ids %s max path ids %s \"%(\n",
    "#                                 link_cost_metric,wk_idx,k,user_pair_id,user_pair,\n",
    "#                                     min(set_of_different_version_of_a_path),\n",
    "#                                     max(set_of_different_version_of_a_path)))\n",
    "                        #if link_cost_metric==\"Hop\":\n",
    "                            #print(\"scheme %s flow %s have these path ids %s \"%(link_cost_metric,user_pair_id,selected_path_for_this_scheme))\n",
    "                            #time.sleep(1)\n",
    "                        try:\n",
    "                            self.each_scheme_each_user_pair_id_paths[link_cost_metric][user_pair_id]=selected_path_for_this_scheme\n",
    "                        except:\n",
    "                            self.each_scheme_each_user_pair_id_paths[link_cost_metric]={}\n",
    "                            self.each_scheme_each_user_pair_id_paths[link_cost_metric][user_pair_id] = selected_path_for_this_scheme\n",
    "\n",
    "                        if not path_flag:# the flow does not have a valid path in this scsheme (path with fidelity higher than 0.6)\n",
    "                            try:\n",
    "                                self.each_scheme_each_user_pair_id_paths[link_cost_metric][user_pair_id]=[]\n",
    "                            except:\n",
    "                                self.each_scheme_each_user_pair_id_paths[link_cost_metric]={}\n",
    "                                self.each_scheme_each_user_pair_id_paths[link_cost_metric][user_pair_id] = []\n",
    "                    if not having_atleast_one_path_flag:#the flow does not have a valid path in this scsheme (path with fidelity higher than 0.6 using any scheme)\n",
    "                        self.each_pair_id_paths[user_pair_id] = []\n",
    "#             print(\"done!\")\n",
    "    def get_paths_between_user_pairs(self,user_pair):\n",
    "        \n",
    "        return self.k_shortest_paths(user_pair[0], user_pair[1], self.cut_off_for_path_searching,\"weight\")\n",
    "    \n",
    "    def check_edge_exit(self,edge):\n",
    "        if edge in self.set_E:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "            \n",
    "    def set_link_weight(self,link_cost_metric):\n",
    "        for edge in self.g.edges:\n",
    "            edge_capacity = self.each_edge_capacity[edge]\n",
    "            if edge in self.set_of_virtual_links or (edge[1],edge[0]) in self.set_of_virtual_links:\n",
    "                weight1=0\n",
    "                weight2 = 0\n",
    "            elif link_cost_metric ==\"Hop\":\n",
    "                weight1=1\n",
    "                weight2 = 1\n",
    "            elif link_cost_metric ==\"EGR\":\n",
    "                weight1=1/edge_capacity\n",
    "                weight2 = 1/edge_capacity\n",
    "            elif link_cost_metric ==\"EGRSquare\":\n",
    "                weight1=1/(edge_capacity**2)\n",
    "                weight2 = 1/(edge_capacity**2)\n",
    "            elif link_cost_metric ==\"Bruteforce\":\n",
    "                weight1=1\n",
    "                weight2 = 1\n",
    "            self.g[edge[0]][edge[1]]['weight']=weight1\n",
    "            self.g[edge[1]][edge[0]]['weight']= weight2\n",
    "            \n",
    "    def update_link_rates(self,alpha_value):\n",
    "        self.alpha_value = alpha_value\n",
    "        edge_rates = []\n",
    "        real_edges_rates = []# this is becasue we want to set the capacity of virtual links to the highest possible rate\n",
    "        for edge in self.g.edges:\n",
    "            edge_length = self.each_edge_distance[edge]\n",
    "            if edge not in self.set_of_virtual_links and (edge[1],edge[0]) not in self.set_of_virtual_links:\n",
    "                c = 1\n",
    "                etha = 10**(-0.1*0.2*edge_length)\n",
    "                T = (edge_length*10**(-4))/25# based on simulation setup of data link layer paper\n",
    "                edge_rate = self.multiplexing_value*(2*c*etha*alpha_value)/T\n",
    "                real_edges_rates.append(edge_rate)\n",
    "        for edge in self.g.edges:\n",
    "            edge_length = self.each_edge_distance[edge]\n",
    "            if edge in self.set_of_virtual_links or (edge[1],edge[0]) in self.set_of_virtual_links:# if it is a virtual link, then set it to highest capacity\n",
    "                edge_rate  = max(real_edges_rates)+1\n",
    "                edge_fidelity = 1 # means virtual links have fidleity 1!\n",
    "            else:\n",
    "                c = 1\n",
    "                etha = 10**(-0.1*0.2*edge_length)\n",
    "                T = (edge_length*10**(-4))/25# based on simulation setup of data link layer paper\n",
    "                edge_rate = self.multiplexing_value*(2*c*etha*alpha_value)/T\n",
    "                edge_fidelity = 1-alpha_value\n",
    "#             edge_rate = 2000\n",
    "            self.g[edge[0]][edge[1]]['capacity']=edge_rate\n",
    "            self.each_edge_capacity[(edge[0],edge[1])] = edge_rate\n",
    "            self.each_edge_capacity[(edge[1],edge[0])] = edge_rate\n",
    "            self.purification.each_edge_fidelity[edge] = edge_fidelity\n",
    "            self.purification.each_edge_fidelity[(edge[1],edge[0])] = edge_fidelity\n",
    "            edge_rates.append(edge_rate)\n",
    "            #print(\"alpha %s generated %s with fidelity %s \"%(alpha_value,edge_rate,1-alpha_value))\n",
    "            \n",
    "        self.max_edge_capacity = max(edge_rates)\n",
    "        self.purification.upper_bound_on_distilation = self.max_edge_capacity+100\n",
    "        \n",
    "    def set_flows_of_organizations(self):\n",
    "        \"\"\"This function reads the workload from the topology+WK file and set the data structures\"\"\"\n",
    "        self.each_wk_k_weight = {}\n",
    "        self.each_wk_organizations={}\n",
    "        self.each_wk_each_k_user_pair_ids = {}\n",
    "        self.each_wk_each_k_user_pairs = {}\n",
    "        self.pair_id = 0\n",
    "        self.each_wk_k_id_pair ={}\n",
    "        self.each_wk_k_pair_id={}\n",
    "        self.work_loads=[]\n",
    "        self.each_wk_k_u_weight ={}\n",
    "        self.each_wk_k_u_pair_weight = {}\n",
    "        self.each_user_pair_id_work_load = {}\n",
    "        self.each_user_pair_id_organization = {}\n",
    "        \"\"\"data structures used for testing the reinforcement learning scheme\"\"\"\n",
    "        self.testing_work_loads=[]\n",
    "        self.each_testing_wk_each_k_user_pairs={}\n",
    "        self.each_testing_wk_each_k_user_pair_ids ={}\n",
    "        self.each_testing_wk_k_weight = {}\n",
    "        self.each_testing_wk_k_u_weight= {}\n",
    "        self.each_testing_wk_organizations = {}\n",
    "        self.each_testing_wk_k_u_pair_weight ={}\n",
    "        \n",
    "        num_nodes = len(self.nodes)\n",
    "        try:\n",
    "            work_load_file = self.topology_file.split(\".txt\")[0]+self.work_load_file_extension\n",
    "        except:\n",
    "            if \".txt\" not in self.topology_file:\n",
    "                work_load_file = self.topology_file+self.work_load_file_extension\n",
    "#         if self.running_path_selection_scheme in [\"EGR\",\"EGRSquare\",\"Hop\",\"Genetic\"]:\n",
    "#             f = open(work_load_file+\"WK2\", 'r')\n",
    "#         else:\n",
    "        f = open(work_load_file, 'r')\n",
    "        self.work_load_counter = 0\n",
    "        all_active_user_pairs_acros_wks = []\n",
    "        header = f.readline()\n",
    "        for line in f:\n",
    "            values = line.strip().split(',')#wk_indx,organization,weight,user_pair,weight\n",
    "            wk_idx = int(values[0])\n",
    "            k = int(values[1])\n",
    "            if k < self.num_of_organizations and (wk_idx<self.workloads_to_test or (self.running_path_selection_scheme ==\"RL\" and wk_idx < self.number_of_training_wks)):\n",
    "                org_weight = float(values[2])\n",
    "                organization_F = float(values[3])\n",
    "                i = int(values[4].split(\":\")[0])\n",
    "                j = int(values[4].split(\":\")[1])\n",
    "                flow_weight = float(values[5])\n",
    "                flow_fidelity_threshold = float(values[6])\n",
    "                user_pair = (i,j)\n",
    "                flow_number_set = int(values[7])\n",
    "                try:\n",
    "                    flow_max_rate_constraint = int(values[8])\n",
    "                except:\n",
    "                    flow_max_rate_constraint = 10000000\n",
    "                if flow_number_set ==self.number_of_flows:\n",
    "                    if wk_idx not in self.work_loads:\n",
    "                        self.work_loads.append(wk_idx)\n",
    "\n",
    "                    \"\"\"we check how many flows we have set for each organization\"\"\"   \n",
    "                    try:\n",
    "                        num_covered_flows = len(self.each_wk_each_k_user_pairs[wk_idx][k])\n",
    "                    except:\n",
    "                        num_covered_flows = 0\n",
    "                    if num_covered_flows<self.number_of_flows:\n",
    "                        try:\n",
    "                            user_pair_id = self.each_pair_id[user_pair] \n",
    "                        except:\n",
    "                            user_pair_id = self.pair_id\n",
    "                            self.pair_id+=1\n",
    "                        self.each_user_pair_id_work_load[user_pair_id] = wk_idx\n",
    "                        self.each_user_pair_id_organization[user_pair_id] = k\n",
    "                        try:\n",
    "                            self.each_wk_each_k_user_pairs[wk_idx][k].append(user_pair)\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_each_k_user_pairs[wk_idx][k]= [user_pair]\n",
    "                            except:\n",
    "                                self.each_wk_each_k_user_pairs[wk_idx]={}\n",
    "                                self.each_wk_each_k_user_pairs[wk_idx][k]=[user_pair]\n",
    "                        \"\"\"we set the maximum rate constraint for this flow \"\"\"\n",
    "                        try:\n",
    "                            self.each_wk_k_u_max_rate_constraint[wk_idx,k,user_pair_id] = flow_max_rate_constraint\n",
    "                        except:\n",
    "                            self.each_wk_k_u_max_rate_constraint[wk_idx,k,user_pair_id] = flow_max_rate_constraint\n",
    "                        \"\"\"we create an id for this flow and added to the data structure\n",
    "                        This is becasue we want to let two organizations have same flows but with different ids\"\"\"\n",
    "                        try:\n",
    "                            self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_k_id_pair[wk_idx][k]={}\n",
    "                                self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "                            except:\n",
    "                                self.each_wk_k_id_pair[wk_idx]={}\n",
    "                                self.each_wk_k_id_pair[wk_idx][k] = {} \n",
    "                                self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "                                \n",
    "                        try:\n",
    "                            self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_k_pair_id[wk_idx][k]={}\n",
    "                                self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "                            except:\n",
    "                                self.each_wk_k_pair_id[wk_idx]={}\n",
    "                                self.each_wk_k_pair_id[wk_idx][k] ={}\n",
    "                                self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "                            \n",
    "                        self.each_user_organization[user_pair_id] = k\n",
    "                        \n",
    "                        # we set the fidelity threshold of the flow\n",
    "                        try:\n",
    "                            self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "                        except:\n",
    "                            try:\n",
    "                                self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]={}\n",
    "                                self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "                            except:\n",
    "                                self.purification.each_wk_k_u_fidelity_threshold[wk_idx]={}\n",
    "                                self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]={} \n",
    "                                self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "                                \n",
    "                        \n",
    "#                         print(\" **** adding pair %s # %s for work load %s for user pair %s ****\"%(user_pair_id,num_covered_flows,wk_idx,user_pair))\n",
    "                        try:\n",
    "                            self.each_wk_each_k_user_pair_ids[wk_idx][k].append(user_pair_id)\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_each_k_user_pair_ids[wk_idx][k]= [user_pair_id]\n",
    "                            except:\n",
    "                                self.each_wk_each_k_user_pair_ids[wk_idx]={}\n",
    "                                self.each_wk_each_k_user_pair_ids[wk_idx][k]= [user_pair_id]\n",
    "                        # we set the weight of each flow\n",
    "                        try:\n",
    "                            self.each_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "                            self.each_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_k_u_weight[wk_idx][k] ={}\n",
    "                                self.each_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "\n",
    "                                self.each_wk_k_u_pair_weight[wk_idx][k] ={}\n",
    "                                self.each_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "                            except:\n",
    "                                self.each_wk_k_u_weight[wk_idx] ={}\n",
    "                                self.each_wk_k_u_weight[wk_idx][k] = {}\n",
    "                                self.each_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "\n",
    "                                self.each_wk_k_u_pair_weight[wk_idx] ={}\n",
    "                                self.each_wk_k_u_pair_weight[wk_idx][k] ={}\n",
    "                                self.each_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "\n",
    "                        \n",
    "                        \"\"\"We set the weight of the organization\"\"\"\n",
    "                        try:\n",
    "                            self.each_wk_k_weight[wk_idx][k] = org_weight\n",
    "                        except:\n",
    "                            self.each_wk_k_weight[wk_idx]= {}\n",
    "                            self.each_wk_k_weight[wk_idx][k] = org_weight  \n",
    "                        # we set the work load its organization\n",
    "                        try:\n",
    "                            if k not in self.each_wk_organizations[wk_idx]:\n",
    "                                try:\n",
    "                                    self.each_wk_organizations[wk_idx].append(k)\n",
    "                                except:\n",
    "                                    self.each_wk_organizations[wk_idx]=[k]\n",
    "                        except:\n",
    "                            self.each_wk_organizations[wk_idx]=[k]\n",
    "#                         print(\"***** we have work load %s for %s ***** \"%(wk_idx,self.running_path_selection_scheme))\n",
    "        \n",
    "                    \n",
    "        \n",
    "        \"\"\"this part is for reinforcement learning scheme.\n",
    "        we involve the data for testing the reinforcment learning scheem in the organization and flow info\n",
    "        \n",
    "        in order to have same state action dimensions in training and testing \"\"\"  \n",
    "#         print(\"********************************** adding testing for RL\",self.workloads_to_test,self.running_path_selection_scheme)\n",
    "#         time.sleep(10)\n",
    "        \n",
    "#         if self.running_path_selection_scheme in [\"RL\"]:\n",
    "#             f = open(work_load_file+\"WK2\", 'r')\n",
    "#             num_covered_flows = 0\n",
    "#             try:\n",
    "#                 work_load_file = self.topology_file.split(\".txt\")[0]\n",
    "#             except:\n",
    "#                 if \".txt\" not in self.topology_file:\n",
    "#                     work_load_file = self.topology_file\n",
    "\n",
    "#             header = f.readline()\n",
    "# #             print(\"we are testing RL and we are adding testing workloads \",self.workloads_to_test)\n",
    "#             for line in f:\n",
    "#                 values = line.strip().split(',')#wk_indx,organization,weight,user_pair,weight\n",
    "#                 wk_idx = int(values[0])\n",
    "#                 k = int(values[1])\n",
    "#                 if wk_idx<self.workloads_to_test and k < self.num_of_organizations:\n",
    "#                     org_weight = float(values[2])\n",
    "#                     organization_F = float(values[3])\n",
    "#                     i = int(values[4].split(\":\")[0])\n",
    "#                     j = int(values[4].split(\":\")[1])\n",
    "#                     flow_weight = float(values[5])\n",
    "#                     flow_fidelity_threshold = float(values[6])\n",
    "#                     user_pair = (i,j)\n",
    "#                     flow_number_set = int(values[7])\n",
    "#                     try:\n",
    "#                         user_pair_id = self.each_pair_id[user_pair] \n",
    "#                     except:\n",
    "#                         user_pair_id = self.pair_id\n",
    "#                         self.pair_id+=1\n",
    "                    \n",
    "#                     if flow_number_set ==self.number_of_flows:\n",
    "#                         if wk_idx not in self.testing_work_loads:\n",
    "#                             self.testing_work_loads.append(wk_idx)\n",
    "\n",
    "#                         \"\"\"we check how many flows we have set for each organization\"\"\"   \n",
    "#                         try:\n",
    "#                             num_covered_flows = len(self.each_testing_wk_each_k_user_pairs[wk_idx][k])\n",
    "#                         except:\n",
    "#                             num_covered_flows = 0\n",
    "#                         if num_covered_flows<self.number_of_flows:\n",
    "#                             try:\n",
    "#                                 self.each_testing_wk_each_k_user_pairs[wk_idx][k].append(user_pair)\n",
    "#                             except:\n",
    "#                                 self.each_testing_wk_each_k_user_pairs[wk_idx]={}\n",
    "#                                 self.each_testing_wk_each_k_user_pairs[wk_idx][k]=[user_pair]\n",
    "\n",
    "#                             \"\"\"we create an id for this flow and added to the data structure\n",
    "#                             This is becasue we want to let two organizations have same flows but with different ids\"\"\"\n",
    "#                             try:\n",
    "#                                 self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "#                             except:\n",
    "#                                 try:\n",
    "#                                     self.each_wk_k_id_pair[wk_idx][k] ={}\n",
    "#                                     self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "#                                 except:\n",
    "#                                     self.each_wk_k_id_pair[wk_idx] ={}\n",
    "#                                     self.each_wk_k_id_pair[wk_idx][k] ={}\n",
    "#                                     self.each_wk_k_id_pair[wk_idx][k][user_pair_id] = user_pair\n",
    "#                             try:\n",
    "#                                 self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "#                             except:\n",
    "#                                 try:\n",
    "#                                     self.each_wk_k_pair_id[wk_idx][k]={}\n",
    "#                                     self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "#                                 except:\n",
    "#                                     self.each_wk_k_pair_id[wk_idx]={}\n",
    "#                                     self.each_wk_k_pair_id[wk_idx][k] ={}\n",
    "#                                     self.each_wk_k_pair_id[wk_idx][k][user_pair] = user_pair_id\n",
    "                                    \n",
    "#                             self.each_testing_user_organization[user_pair_id] = k\n",
    "#                             self.each_user_pair_id_work_load[user_pair_id] = wk_idx\n",
    "#                             self.each_user_pair_id_organization[user_pair_id] = k\n",
    "                        \n",
    "#     #                         print(\" **** adding pair %s # %s for work load %s for user pair %s ****\"%(user_pair_id,num_covered_flows,wk_idx,user_pair))\n",
    "#                             try:\n",
    "#                                 self.each_testing_wk_each_k_user_pair_ids[wk_idx][k].append(user_pair_id)\n",
    "#                             except:\n",
    "#                                 try:\n",
    "#                                     self.each_testing_wk_each_k_user_pair_ids[wk_idx][k]= [user_pair_id]\n",
    "#                                 except:\n",
    "#                                     self.each_testing_wk_each_k_user_pair_ids[wk_idx]={}\n",
    "#                                     self.each_testing_wk_each_k_user_pair_ids[wk_idx][k]= [user_pair_id]\n",
    "#                             # we set the weight of each flow\n",
    "#                             try:\n",
    "#                                 self.each_testing_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "#                                 self.each_testing_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "#                             except:\n",
    "#                                 try:\n",
    "#                                     self.each_testing_wk_k_u_weight[wk_idx][k] ={}\n",
    "#                                     self.each_testing_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "\n",
    "#                                     self.each_testing_wk_k_u_pair_weight[wk_idx][k] ={}\n",
    "#                                     self.each_testing_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "#                                 except:\n",
    "#                                     self.each_testing_wk_k_u_weight[wk_idx] ={}\n",
    "#                                     self.each_testing_wk_k_u_weight[wk_idx][k] = {}\n",
    "#                                     self.each_testing_wk_k_u_weight[wk_idx][k][user_pair_id] = flow_weight\n",
    "\n",
    "#                                     self.each_testing_wk_k_u_pair_weight[wk_idx] ={}\n",
    "#                                     self.each_testing_wk_k_u_pair_weight[wk_idx][k] ={}\n",
    "#                                     self.each_testing_wk_k_u_pair_weight[wk_idx][k][user_pair] = flow_weight\n",
    "\n",
    "#                             # we set the fidelity threshold of the flow\n",
    "#                             try:\n",
    "#                                 self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "#                             except:\n",
    "#                                 try:\n",
    "#                                     self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]={}\n",
    "#                                     self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "#                                 except:\n",
    "#                                     self.purification.each_wk_k_u_fidelity_threshold[wk_idx]={}\n",
    "#                                     self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]={} \n",
    "#                                     self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id] =flow_fidelity_threshold \n",
    "                                \n",
    "#                             \"\"\"We set the weight of the organization\"\"\"\n",
    "#                             try:\n",
    "#                                 self.each_testing_wk_k_weight[wk_idx][k] = org_weight\n",
    "#                             except:\n",
    "#                                 self.each_testing_wk_k_weight[wk_idx]= {}\n",
    "#                                 self.each_testing_wk_k_weight[wk_idx][k] = org_weight  \n",
    "#                             # we set the work load its organization\n",
    "#                             try:\n",
    "#                                 if k not in self.each_testing_wk_organizations[wk_idx]:\n",
    "#                                     try:\n",
    "#                                         self.each_testing_wk_organizations[wk_idx].append(k)\n",
    "#                                     except:\n",
    "#                                         self.each_testing_wk_organizations[wk_idx]=[k]\n",
    "#                             except:\n",
    "#                                 self.each_testing_wk_organizations[wk_idx]=[k]\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    def set_each_k_user_pair_paths(self,wk_idx):\n",
    "        \"\"\"we set self.num_of_paths for each user pair of each organization \"\"\"\n",
    "        self.path_counter_id  = 0\n",
    "        added_paths = []\n",
    "        self.each_wk_each_k_each_user_pair_id_all_paths={}\n",
    "        self.each_wk_each_k_each_user_pair_id_paths = {}\n",
    "        for k,user_pair_ids in self.each_wk_each_k_user_pair_ids[wk_idx].items():\n",
    "            for user_pair_id in user_pair_ids:\n",
    "                user_pair = self.each_wk_k_id_pair[wk_idx][k][user_pair_id]\n",
    "#                 print(\"for org %s flow id %s pair %s \"%(k,user_pair_id,user_pair))\n",
    "                having_at_least_one_path_flag = False\n",
    "                one_path_added_flag = False\n",
    "                for path in self.k_shortest_paths(user_pair[0], user_pair[1], self.num_of_paths,\"weight\"):\n",
    "                    node_indx = 0\n",
    "                    path_edges = []\n",
    "                    for node_indx in range(len(path)-1):\n",
    "                        path_edges.append((path[node_indx],path[node_indx+1]))\n",
    "                        node_indx+=1\n",
    "#                     flag= False\n",
    "#                     for p_id,edges in self.set_of_paths.items():\n",
    "#                         if edges == path_edges:\n",
    "#                             path_id2 = p_id\n",
    "#                             flag = True\n",
    "#                     if flag:\n",
    "#                         path_id=path_id2\n",
    "                    path_fidelity = self.purification.get_fidelity(path_edges,self.set_of_virtual_links)\n",
    "                    #if path_fidelity>0.5:# the condition of 1==1 is for the reasont hat we want to use all paths\n",
    "                    #if not flag:\n",
    "#                     pur_scheme  = self.purification.purification_schemes[0]\n",
    "                \n",
    "                    if self.distilation_scheme_for_huristic==\"random\":\n",
    "                        size_of_distilation_strategies = len(self.purification.set_of_edge_level_Fth)\n",
    "                        edge_level_Fth = self.purification.set_of_edge_level_Fth[random.randint(0,(size_of_distilation_strategies-1))]\n",
    "                    elif self.distilation_scheme_for_huristic==\"edge-level\":\n",
    "                        edge_level_Fth = 1\n",
    "                    elif self.distilation_scheme_for_huristic==\"end-level\":\n",
    "                        edge_level_Fth=0\n",
    "                    else:\n",
    "                        size_of_distilation_strategies = len(self.purification.set_of_edge_level_Fth)\n",
    "                        edge_level_Fth = self.purification.set_of_edge_level_Fth[random.randint(0,(size_of_distilation_strategies-1))]\n",
    "                    self.purification.each_path_edge_level_Fth[self.path_counter_id]=edge_level_Fth\n",
    "\n",
    "                    path_id = self.path_counter_id\n",
    "                    added_paths.append(path_edges)\n",
    "                    self.set_each_path_length(self.path_counter_id,path_edges)\n",
    "                    self.set_of_paths[self.path_counter_id] = path_edges\n",
    "#                         self.purification.each_path_id_purificaiton_scheme[self.path_counter_id] = pur_scheme\n",
    "                    #we set the basic fidelity of path here\n",
    "                    self.purification.each_path_basic_fidelity[path_id]= round(path_fidelity,3)\n",
    "                    #print(\"these are the user pair ids in wk %s k %s: %s\"%(wk_idx,k,self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k]))\n",
    "#                         print(\"2 we set Fth %s for wk %s k %s path %s from flow %s \"%(self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id],wk_idx,k,self.path_counter_id,user_pair_id))\n",
    "                    self.purification.each_path_flow_target_fidelity[self.path_counter_id] = self.purification.each_wk_k_u_fidelity_threshold[wk_idx][k][user_pair_id]\n",
    "                    try:\n",
    "                        if len(self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id])<self.num_of_paths:\n",
    "                            having_at_least_one_path_flag = True\n",
    "                            one_path_added_flag=True\n",
    "                            try:\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id].append(path_id)\n",
    "                            except:\n",
    "                                try:\n",
    "                                    self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                                    except:\n",
    "                                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx]={}\n",
    "                                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                    except:\n",
    "                        having_at_least_one_path_flag = True\n",
    "                        one_path_added_flag=True\n",
    "                        try:\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                        except:\n",
    "                            try:\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                            except:\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx]={}\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                                self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                    self.path_counter_id+=1\n",
    "                if not having_at_least_one_path_flag:\n",
    "                    try:\n",
    "                        self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = []\n",
    "                    except:\n",
    "                        try:\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = []\n",
    "                        except:\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id] = []\n",
    "\n",
    "#                     shortest_disjoint_paths = nx.edge_disjoint_paths(self.g,s=user_pair[0],t=user_pair[1])             \n",
    "        \n",
    "    def k_shortest_paths(self, source, target, k, weight):\n",
    "        return list(\n",
    "            islice(nx.shortest_simple_paths(self.g, source, target, weight=weight), k)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    def set_distillable_paths(self,wk_idx):\n",
    "        \"\"\"here we exclude the paths that after edge-level distilation have e2e fidleity less than 0.5\"\"\"\n",
    "        \n",
    "        self.each_wk_each_k_each_user_pair_id_paths = {}\n",
    "        \n",
    "        for k,user_pair_ids in self.each_wk_each_k_user_pair_ids[wk_idx].items():\n",
    "            for user_pair_id in user_pair_ids:\n",
    "                user_pair = self.each_wk_k_id_pair[wk_idx][k][user_pair_id]\n",
    "                having_at_least_one_path_flag = False\n",
    "                one_path_added_flag = False\n",
    "                all_path_ids = self.each_wk_each_k_each_user_pair_id_all_paths[wk_idx][k][user_pair_id]\n",
    "                for path_id in all_path_ids:\n",
    "                    if self.purification.check_path_distilability(path_id):\n",
    "                        try:\n",
    "                            if len(self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id])<self.num_of_paths:\n",
    "                                having_at_least_one_path_flag = True\n",
    "                                one_path_added_flag=True\n",
    "                                try:\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id].append(path_id)\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                                        except:\n",
    "                                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx]={}\n",
    "                                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                        except:\n",
    "                            having_at_least_one_path_flag = True\n",
    "                            one_path_added_flag=True\n",
    "                            try:\n",
    "                                self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                            except:\n",
    "                                try:\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                                except:\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx]={}\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                                    self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = [path_id]\n",
    "                if not having_at_least_one_path_flag:\n",
    "                    try:\n",
    "                        self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = []\n",
    "                    except:\n",
    "                        try:\n",
    "                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = []\n",
    "                        except:\n",
    "                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k]={}\n",
    "                            self.each_wk_each_k_each_user_pair_id_paths[wk_idx][k][user_pair_id] = []\n",
    "\n",
    "    \n",
    "    def set_paths_in_the_network(self,wk_idx):\n",
    "        self.reset_pair_paths()\n",
    "        self.set_each_k_user_pair_paths(wk_idx)\n",
    "        \"\"\"we set the required EPR pairs to achieve each fidelity threshold\"\"\"\n",
    "        self.purification.set_required_EPR_pairs_for_distilation(wk_idx,self)\n",
    "        self.set_distillable_paths(wk_idx)\n",
    "       \n",
    "    def reset_pair_paths(self):\n",
    "        self.path_id = 0\n",
    "        self.path_counter_id = 0\n",
    "        self.set_of_paths = {}\n",
    "        self.purification.each_path_version_numbers= {}\n",
    "        self.purification.each_path_id_purificaiton_scheme = {}\n",
    "        self.each_k_u_paths = {}\n",
    "        self.each_k_u_disjoint_paths = {}\n",
    "        self.each_k_path_path_id = {}\n",
    "        self.each_k_u_all_paths = {}\n",
    "        self.each_k_u_all_disjoint_paths = {}\n",
    "        self.each_k_path_path_id={}\n",
    "        \n",
    "        # these are purification object data sctructures the we reset\n",
    "        self.purification.each_path_basic_fidelity ={}\n",
    "        self.purification.all_basic_fidelity_target_thresholds = []\n",
    "        self.purification.each_n_f_purification_result = {}\n",
    "        self.purification.each_edge_target_fidelity = {}\n",
    "        self.purification.function_g_computed  = False\n",
    "        self.purification.oracle_for_edge_target_fidelity = {}\n",
    "        self.purification.oracle_for_target_fidelity = {}\n",
    "        self.purification.each_path_version_numbers= {}\n",
    "        self.purification.each_path_id_purificaiton_scheme = {}\n",
    "        self.purification.each_path_flow_target_fidelity = {}\n",
    "        self.purification.each_path_target_fidelity_for_edge_dislication = {}\n",
    "        self.purification.unable_to_distill_paths =[]\n",
    "    def set_each_user_pair_demands(self):\n",
    "        self.each_t_each_request_demand = {}\n",
    "        num_of_pairs= len(list(self.user_pairs))\n",
    "        tm = spike_tm(num_of_pairs+1,num_spikes,spike_mean,1)\n",
    "        \n",
    "        traffic = tm.at_time(1)\n",
    "        printed_pairs = []\n",
    "        user_indx = 0\n",
    "        for i in range(num_of_pairs):\n",
    "            for j in range(num_of_pairs):\n",
    "                if i!=j:\n",
    "                    if (i,j) not in printed_pairs and (j,i) not in printed_pairs and user_indx<num_of_pairs:\n",
    "                        printed_pairs.append((i,j))\n",
    "                        printed_pairs.append((j,i))\n",
    "#                             print(\"num_of_pairs %s time %s traffic from %s to %s is %s and user_indx %s\"%(num_of_pairs, time,i,j,traffic[i][j],user_indx))\n",
    "                        request = user_pairs[time][user_indx]\n",
    "                        user_indx+=1\n",
    "                        demand = max(1,traffic[i][j])\n",
    "                        try:\n",
    "                            self.each_u_request_demand[request] = demand\n",
    "                        except:\n",
    "                            self.each_u_demand[request] = demand\n",
    "        for request in self.user_pairs[time]:\n",
    "            try:\n",
    "                self.each_u_demand[0][request] = 0\n",
    "            except:\n",
    "                self.each_u_demand[0]={}\n",
    "                self.each_u_demand[0][request] = 0\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    def reset_variables(self):\n",
    "        self.each_wk_k_id_pair ={}\n",
    "        self.pair_id = 0\n",
    "        self.max_edge_capacity = int(config.max_edge_capacity)\n",
    "        self.min_edge_capacity = int(config.min_edge_capacity)\n",
    "        self.min_edge_fidelity = float(config.min_edge_fidelity)\n",
    "        self.max_edge_fidelity = float(config.max_edge_fidelity)\n",
    "        self.num_of_paths = int(config.num_of_paths)\n",
    "        self.path_selection_scheme = config.path_selection_scheme\n",
    "        self.each_pair_id  ={}\n",
    "       \n",
    "        self.set_of_paths = {}\n",
    "        self.each_u_paths = {}\n",
    "        self.purificaiton.each_n_f_purification_result = {}\n",
    "        self.purificaiton.each_edge_target_fidelity = {}\n",
    "        self.each_u_all_real_paths = {}\n",
    "        self.each_u_all_real_disjoint_paths = {}\n",
    "        self.each_u_paths = {}\n",
    "        self.nodes = []\n",
    "        self.purificaiton.oracle_for_target_fidelity = {}\n",
    "        self.each_k_path_path_id = {}\n",
    "        self.purificaiton.global_each_basic_fidelity_target_fidelity_required_EPRs = {}\n",
    "        self.purificaiton.purificaiton.all_basic_fidelity_target_thresholds = []\n",
    "        self.path_counter_id = 0\n",
    "        self.pair_id = 0\n",
    "        self.each_u_weight={}\n",
    "        self.each_path_legth = {}\n",
    "        self.load_topology()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def set_each_path_length(self,path_id,path_edges):\n",
    "        path_length = 0\n",
    "        for edge in path_edges:\n",
    "            if edge not in self.set_of_virtual_links and (edge[1],edge[0]) not in self.set_of_virtual_links:\n",
    "                path_length+=1\n",
    "        self.each_path_legth[path_id] = path_length\n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_real_longest_path(self,user_or_storage_pair,number_of_paths):\n",
    "        all_paths=[]\n",
    "        for path in nx.all_simple_paths(self.g,source=user_or_storage_pair[0],target=user_or_storage_pair[1]):\n",
    "            #all_paths.append(path)\n",
    "\n",
    "            node_indx = 0\n",
    "            path_edges = []\n",
    "            for node_indx in range(len(path)-1):\n",
    "                path_edges.append((path[node_indx],path[node_indx+1]))\n",
    "                node_indx+=1\n",
    "            all_paths.append(path_edges)\n",
    "\n",
    "        all_paths.sort(key=len,reverse=True)\n",
    "        if len(all_paths)>=number_of_paths:\n",
    "            return all_paths[:number_of_paths]\n",
    "        else:\n",
    "            return all_paths\n",
    "                        \n",
    "    def get_real_path(self,user_or_storage_pair_id):\n",
    "        if self.path_selection_scheme==\"shortest\":\n",
    "            path_selecion_flag = False\n",
    "            path_counter = 1\n",
    "            paths = []\n",
    "            #print(\"user_or_storage_pair\",user_or_storage_pair)\n",
    "            #print(\"self.each_user_pair_all_real_paths[user_or_storage_pair]\",self.each_user_pair_all_real_paths[user_or_storage_pair])\n",
    "            for path in self.each_user_pair_all_real_paths[user_or_storage_pair_id]:\n",
    "                #print(\"we can add this path\",path)\n",
    "                if path_counter<=self.num_of_paths:\n",
    "                    node_indx = 0\n",
    "                    path_edges = []\n",
    "                    for node_indx in range(len(path)-1):\n",
    "                        path_edges.append((path[node_indx],path[node_indx+1]))\n",
    "                        node_indx+=1\n",
    "                    paths.append(path_edges)\n",
    "\n",
    "                path_counter+=1\n",
    "        elif self.path_selection_scheme==\"shortest_disjoint\":\n",
    "            path_selecion_flag = False\n",
    "            path_counter = 1\n",
    "            paths = []\n",
    "            #print(\"self.each_user_pair_all_real_paths[user_or_storage_pair]\",self.each_user_pair_all_real_paths[user_or_storage_pair])\n",
    "            for path in self.each_user_pair_all_real_disjoint_paths[user_or_storage_pair_id]:\n",
    "                #print(\"we can add this path\",path)\n",
    "                if path_counter<=self.num_of_paths:\n",
    "                    node_indx = 0\n",
    "                    path_edges = []\n",
    "                    for node_indx in range(len(path)-1):\n",
    "                        path_edges.append((path[node_indx],path[node_indx+1]))\n",
    "                        node_indx+=1\n",
    "                    paths.append(path_edges)\n",
    "\n",
    "                path_counter+=1\n",
    "            \n",
    "        return paths\n",
    "                    \n",
    "      \n",
    "                    \n",
    "    \n",
    "   \n",
    "    def get_edges(self):\n",
    "        return self.set_E\n",
    "    \n",
    "\n",
    "    def check_path_include_edge(self,edge,path):\n",
    "        \n",
    "        if edge in self.set_of_paths[path]:\n",
    "            return True\n",
    "        elif edge not  in self.set_of_paths[path]:\n",
    "            return False\n",
    "\n",
    "    def check_request_use_path(self,k,p):\n",
    "        if p in self.each_u_paths[k]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def get_path_length(self,path):\n",
    "        return self.each_path_legth[path]-1\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
